{
 "cells": [
  {
   "cell_type": "code",
   "id": "a014f4bae30f6c30",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T21:25:31.325789Z",
     "start_time": "2024-07-16T21:25:31.272912Z"
    }
   },
   "source": [
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "import seaborn as sns\n",
    "import networkx as nx\n",
    "import community as community_louvain\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from pyvis.network import Network\n",
    "import plotly.express as px\n",
    "import os\n",
    "\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 37
  },
  {
   "cell_type": "code",
   "id": "626ba9d0c21a1049",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T21:25:32.943775Z",
     "start_time": "2024-07-16T21:25:32.860552Z"
    }
   },
   "source": [
    "def normalize_topics_relation_weights(df: pd.DataFrame):\n",
    "    for index, row in df.iterrows():\n",
    "        if len(row['target']) > 0:\n",
    "            sum_num = sum([i['count'] for i in row['target']])\n",
    "            if len(row['target']) == 1:\n",
    "                row['target'][0]['count'] = 1\n",
    "            else:\n",
    "                for index_count, count in enumerate(row['target']):\n",
    "                    normalized_number = (count['count'] / sum_num)\n",
    "                    row['target'][index_count]['count'] = normalized_number\n",
    "        df.iloc[index] = row\n"
   ],
   "outputs": [],
   "execution_count": 38
  },
  {
   "cell_type": "markdown",
   "id": "5208328fa5f4e614",
   "metadata": {},
   "source": [
    "# Find Topics relations with direction: Tweet Topic -> Reply/Quote Topic"
   ]
  },
  {
   "cell_type": "code",
   "id": "c1e96424c79feb09",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T21:25:33.007978Z",
     "start_time": "2024-07-16T21:25:32.966016Z"
    }
   },
   "source": [
    "\n",
    "def find_topics_by_tweets(inputDataFrame: pd.DataFrame):\n",
    "    df = pd.DataFrame(columns=['source', 'target'])\n",
    "    tweets_ids = []\n",
    "    for index, topic_id in enumerate(set(inputDataFrame['topics'].tolist())):\n",
    "        tweets_id = inputDataFrame[inputDataFrame['topics'] == topic_id]['id'].tolist()\n",
    "        tweets_ids = tweets_ids + tweets_id\n",
    "        replies_topics = replies_df[replies_df['in_reply_to_tweet_id'].isin(tweets_id)]['topics'].tolist()\n",
    "        quotes_topics = quotes_df[quotes_df['quoted_tweet_id'].isin(tweets_id)]['topics'].tolist()\n",
    "        replies_quotes_topics = replies_topics + quotes_topics\n",
    "        replies_quotes_topics_dict = []\n",
    "        for replies_quotes_topic in set(replies_quotes_topics):\n",
    "            count = replies_quotes_topics.count(replies_quotes_topic)\n",
    "            replies_quotes_topics_dict.append({'count': count,\n",
    "                                               'target_topic': replies_quotes_topic})\n",
    "        df.loc[len(df)] = {'source': topic_id, 'target': replies_quotes_topics_dict}\n",
    "    normalize_topics_relation_weights(df)\n",
    "    replies_topics = replies_df[replies_df['in_reply_to_tweet_id'].isin(tweets_ids)]\n",
    "    quotes_topics = quotes_df[quotes_df['quoted_tweet_id'].isin(tweets_ids)]\n",
    "\n",
    "    return df, find_topics_by_replies_and_quotes(inputDataFrame, replies_topics, quotes_topics)\n",
    "\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 39
  },
  {
   "cell_type": "markdown",
   "id": "ff93e9c5e51c6aa",
   "metadata": {},
   "source": [
    "# Find Topics relations with direction: Reply/Quote Topic  -> Tweet Topic"
   ]
  },
  {
   "cell_type": "code",
   "id": "1ec67888c401aabb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T21:25:33.275553Z",
     "start_time": "2024-07-16T21:25:33.253660Z"
    }
   },
   "source": [
    "def find_topics_by_replies_and_quotes(tweets_df: pd.DataFrame, replies_df: pd.DataFrame, quotes_df: pd.DataFrame):\n",
    "    df = pd.DataFrame(columns=['source', 'target'])\n",
    "    topics_replies_quotes = (quotes_df['topics'].tolist() + replies_df['topics'].tolist())[:10_00]\n",
    "    df = pd.DataFrame(columns=['source', 'target'])\n",
    "    for index, topic_id in enumerate(set(topics_replies_quotes)):\n",
    "        replies_id = replies_df[replies_df['topics'] == topic_id]['in_reply_to_tweet_id'].tolist()\n",
    "        quotes_id = quotes_df[quotes_df['topics'] == topic_id]['quoted_tweet_id'].tolist()\n",
    "        replies_quotes_id = replies_id + quotes_id\n",
    "        tweets_topics = tweets_df[tweets_df['id'].isin(replies_quotes_id)]['topics'].tolist()\n",
    "        replies_quotes_topics_dict = []\n",
    "        for tweet_topic in set(tweets_topics):\n",
    "            count = tweets_topics.count(tweet_topic)\n",
    "            replies_quotes_topics_dict.append({'count': count,\n",
    "                                               'target_topic': tweet_topic})\n",
    "        df.loc[len(df)] = {'source': topic_id, 'target': replies_quotes_topics_dict}\n",
    "    normalize_topics_relation_weights(df)\n",
    "    return df"
   ],
   "outputs": [],
   "execution_count": 40
  },
  {
   "cell_type": "markdown",
   "id": "87eb8a448e4d1ea7",
   "metadata": {},
   "source": [
    "# Function to calculate conditional probability for transitions between topics for each user monthly"
   ]
  },
  {
   "cell_type": "code",
   "id": "e92d530146bf0d07",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T21:25:33.463225Z",
     "start_time": "2024-07-16T21:25:33.386018Z"
    }
   },
   "source": [
    "from itertools import product\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "def calculate_conditional_probability_for_topics_transitions_between_users():\n",
    "    \"\"\"\n",
    "    tweets_df['month'] = pd.to_datetime(tweets_df['date']).dt.to_period('M')\n",
    "    # List of unique topics\n",
    "    topics = tweets_df['topics'].unique()\n",
    "\n",
    "    # List of unique months\n",
    "    months = tweets_df['month'].unique()\n",
    "\n",
    "    # Prepare DataFrame to store conditional probabilities\n",
    "    conditional_probs = []\n",
    "\n",
    "    # Loop through all combinations of topics and months\n",
    "    for topic_x, topic_y, month_x, month_y in product(topics, topics, months, months):\n",
    "        if month_y <= month_x:\n",
    "            continue\n",
    "\n",
    "        # Identify users who mentioned Topic X in Month 1\n",
    "        users_topic_x_month_x = tweets_df[(tweets_df['topics'] == topic_x) & (tweets_df['month'] == month_x)][\n",
    "            'user_id'].unique()\n",
    "\n",
    "        # Identify how many of these users mentioned Topic Y in Month 2\n",
    "        users_topic_y_month_y = tweets_df[\n",
    "            (tweets_df['user_id'].isin(users_topic_x_month_x)) & (tweets_df['topics'] == topic_y) & (\n",
    "                    tweets_df['month'] == month_y)]['user_id'].unique()\n",
    "\n",
    "        # Calculate the conditional probability\n",
    "        prob_topic_y_given_topic_x = len(users_topic_y_month_y) / len(users_topic_x_month_x) if len(\n",
    "            users_topic_x_month_x) > 0 else 0\n",
    "\n",
    "        # Store the result\n",
    "        conditional_probs.append([topic_x, topic_y, month_x, month_y, prob_topic_y_given_topic_x])\n",
    "\n",
    "    # Create DataFrame for visualization\n",
    "    viz_df = pd.DataFrame(conditional_probs, columns=['topic_x', 'topic_y', 'month_x', 'month_y', 'probability'])\n",
    "\n",
    "    # Filter out zero probabilities for visualization purposes\n",
    "    viz_df = viz_df[viz_df['probability'] > 0]\n",
    "\n",
    "    print(viz_df.to_dict('records'))\n",
    "    viz_df['topic_x'] = viz_df['topic_x'].astype(str)\n",
    "    viz_df['topic_y'] = viz_df['topic_y'].astype(str)\n",
    "    viz_df['topic_x_y'] = viz_df['topic_x'].str.cat(viz_df['topic_y'], sep=' - ')\n",
    "    viz_df['month_x'] = viz_df['month_x'].astype(str)\n",
    "    viz_df['month_y'] = viz_df['month_y'].astype(str)\n",
    "    viz_df['month_x_y'] = viz_df['month_x'].str.cat(viz_df['month_y'], sep=' - ')\n",
    "    viz_df.drop_duplicates(inplace=True)\n",
    "\n",
    "    # Create figure with plotly.graph_objects and set width and height\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # Add scatter plot with markers\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=viz_df['topic_x_y'],\n",
    "        y=viz_df['month_x_y'],\n",
    "        mode='markers+text',  # Display as scatter plot with text labels\n",
    "\n",
    "        # marker=dict(\n",
    "        #     size=viz_df['probability'],  # Scale the marker size\n",
    "        #    sizemin=10\n",
    "        #),\n",
    "        text=viz_df.apply(lambda row: f\"Prob: {row['probability']:.2f}<br>Mx: {row['month_x']} My: {row['month_y']}\",\n",
    "                          axis=1),\n",
    "        textposition='top center',\n",
    "        name='Markers'\n",
    "    ))\n",
    "\n",
    "    # Set x and y axes to show only integer ticks, including negatives\n",
    "    fig.update_xaxes(tickmode='linear', dtick=1, title_text='Topics Combination')\n",
    "    fig.update_yaxes(tickmode='linear', dtick=1, title_text='Dates Combination')\n",
    "\n",
    "    # Set plot title and layout\n",
    "    fig.update_layout(\n",
    "        title={\n",
    "            'text': 'Conditional Probability of Topic Transitions',\n",
    "            'y': 0.9,\n",
    "            'x': 0.5,\n",
    "            'xanchor': 'center',\n",
    "            'yanchor': 'top'\n",
    "        },\n",
    "    )\n",
    "\n",
    "    # Show and save plot\n",
    "    fig.write_html('Conditional Probability of Topic Transitions.html')\n",
    "    fig.show()\n",
    "    \n",
    "    \"\"\"\n",
    "    tweets_df['month'] = pd.to_datetime(tweets_df['date']).dt.to_period('M')\n",
    "    probabilities = {}\n",
    "    topics_months = tweets_df[['topics', 'month']].drop_duplicates().values\n",
    "    # Deduplicate the DataFrame based on user_id, topic, and month\n",
    "    df = tweets_df.drop_duplicates(subset=['user_id', 'topics', 'month'])\n",
    "\n",
    "    for (topic_x, month_x) in topics_months:\n",
    "        users_x = set(df[(df['topics'] == topic_x) & (df['month'] == month_x)]['user_id'])\n",
    "\n",
    "        for (topic_y, month_y) in topics_months:\n",
    "            if month_y > month_x:  # Ensure Y is after X\n",
    "                users_y = set(df[(df['topics'] == topic_y) & (df['month'] == month_y)]['user_id'])\n",
    "                common_users = users_x.intersection(users_y)\n",
    "                prob = len(common_users) / len(users_x) if len(users_x) > 0 else 0\n",
    "\n",
    "                probabilities[((topic_x, month_x), (topic_y, month_y))] = prob\n",
    "\n",
    "    print(probabilities.items())\n",
    "    print('probability ')\n",
    "    # Create the network graph\n",
    "    G = nx.Graph()\n",
    "\n",
    "    for (node1, node2), prob in probabilities.items():\n",
    "        if prob > 0:  # Add only edges with non-zero probability\n",
    "            G.add_edge(node1, node2, weight=prob)\n",
    "\n",
    "    # Louvain community detection\n",
    "    partition = community_louvain.best_partition(G)\n",
    "    \"\"\"\n",
    "    # Draw the network graph\n",
    "    pos = nx.spring_layout(G)\n",
    "    edges = G.edges(data=True)\n",
    "\n",
    "    weights = [edge[2]['weight'] for edge in edges]\n",
    "    nx.draw(G, pos, with_labels=True, node_size=700, node_color='skyblue', font_size=10, font_weight='bold',\n",
    "            arrows=True, edge_color=weights, edge_cmap=plt.colormaps.get_cmap(\"Spectral\"))\n",
    "    edge_labels = {(u, v): f'{d[\"weight\"]:.2f}' for u, v, d in edges}\n",
    "    nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels)\n",
    "    \"\"\"\n",
    "\n",
    "    #plt.title('User Participation Network')\n",
    "    #plt.show()\n",
    "    # Convert NetworkX graph to Pyvis network\n",
    "    net = Network(notebook=True, height=\"750px\", width=\"100%\", bgcolor=\"#222222\", font_color=\"white\")\n",
    "    print(G.nodes)\n",
    "    # Add nodes and edges from NetworkX graph to Pyvis network\n",
    "    for node in G.nodes():\n",
    "        net.add_node(f'{node[0]}_{str(node[1])}', label=str(node), title=str(node), size=20)\n",
    "\n",
    "    for edge in G.edges(data=True):\n",
    "        net.add_edge(f'{edge[0][0]}_{str(edge[0][1])}', f'{edge[1][0]}_{str(edge[1][1])}', value=edge[2]['weight'],\n",
    "                     title=f'Weight: {edge[2][\"weight\"]:.2f}')\n",
    "\n",
    "    # Customize options\n",
    "    net.set_options(\"\"\"\n",
    "var options = {\n",
    "  \"nodes\": {\n",
    "    \"font\": {\n",
    "      \"size\": 12,\n",
    "      \"face\": \"arial\",\n",
    "      \"color\": \"white\"\n",
    "    },\n",
    "    \"color\": {\n",
    "      \"background\": \"#97C2FC\",\n",
    "      \"border\": \"#2B7CE9\"\n",
    "    }\n",
    "  },\n",
    "  \"edges\": {\n",
    "    \"color\": {\n",
    "      \"color\": \"#AAAAAA\",\n",
    "      \"highlight\": \"#FFD700\",\n",
    "      \"hover\": \"#FFD700\"\n",
    "    },\n",
    "    \"width\": 2,\n",
    "    \"arrows\": {\n",
    "      \"to\": {\n",
    "        \"enabled\": true,\n",
    "        \"type\": \"arrow\"\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\"\"\")\n",
    "\n",
    "    # Show the network\n",
    "    net.show(\"user_participation_network.html\", notebook=False)\n",
    "    # Pyvis graph for community detection\n",
    "    net_community = Network(height=\"750px\", width=\"100%\", bgcolor=\"#222222\", font_color=\"white\")\n",
    "\n",
    "    # Add nodes to community graph\n",
    "    for node, community_id in partition.items():\n",
    "        net_community.add_node(f'{node[0]}_{str(node[1])}', label=str(node), title=str(node), size=20,\n",
    "                               color=community_id)\n",
    "\n",
    "    # Add edges to community graph (only add edges between nodes in the same community)\n",
    "    for edge in G.edges():\n",
    "        if partition[edge[0]] == partition[edge[1]]:\n",
    "            net_community.add_edge(f'{edge[0][0]}_{str(edge[0][1])}', f'{edge[1][0]}_{str(edge[1][1])}')\n",
    "\n",
    "    # Customize options for community graph\n",
    "    net_community.set_options(\"\"\"\n",
    "    var options = {\n",
    "  \"nodes\": {\n",
    "    \"font\": {\n",
    "      \"size\": 12,\n",
    "      \"face\": \"arial\",\n",
    "      \"color\": \"white\"\n",
    "    }\n",
    "  },\n",
    "  \"edges\": {\n",
    "    \"color\": {\n",
    "      \"color\": \"#AAAAAA\",\n",
    "      \"highlight\": \"#FFD700\",\n",
    "      \"hover\": \"#FFD700\"\n",
    "    },\n",
    "    \"width\": 2,\n",
    "    \"arrows\": {\n",
    "      \"to\": {\n",
    "        \"enabled\": true,\n",
    "        \"type\": \"arrow\"\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\"\"\")\n",
    "\n",
    "    #  Save both graphs to HTML files\n",
    "    net_community.show(\"user_participation_network_community_detection.html\", notebook=False)\n"
   ],
   "outputs": [],
   "execution_count": 41
  },
  {
   "cell_type": "code",
   "id": "490203ccd93d28f7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T21:25:33.557061Z",
     "start_time": "2024-07-16T21:25:33.479579Z"
    }
   },
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "def calculate_conditional_probability_for_topics_transitions_between_users_():\n",
    "    tweets_df['month'] = pd.to_datetime(tweets_df['date']).dt.to_period('M')\n",
    "    # Ensure DataFrame is sorted by user_id and month\n",
    "    df = tweets_df.sort_values(by=['user_id', 'month'])\n",
    "\n",
    "    # Create a dictionary to hold topic transitions\n",
    "    transitions = defaultdict(lambda: defaultdict(int))\n",
    "    topic_counts = defaultdict(int)\n",
    "\n",
    "    # Populate the dictionary with topic transitions across all users\n",
    "    for user_id in df['user_id'].unique():\n",
    "        user_data = df[df['user_id'] == user_id]\n",
    "        previous_topic = None\n",
    "        previous_month = None\n",
    "\n",
    "        for _, row in user_data.iterrows():\n",
    "            current_topic = row['topics']\n",
    "            current_month = row['month']\n",
    "\n",
    "            if previous_topic is not None and previous_month != current_month:\n",
    "                transitions[(previous_topic, previous_month, user_id)][(current_topic, current_month, user_id)] += 1\n",
    "                topic_counts[(previous_topic, previous_month, user_id)] += 1\n",
    "\n",
    "            previous_topic = current_topic\n",
    "            previous_month = current_month\n",
    "\n",
    "    #   Calculate conditional probabilities\n",
    "    conditional_probabilities = defaultdict(lambda: defaultdict(float))\n",
    "    for key in transitions:\n",
    "        for next_key in transitions[key]:\n",
    "            conditional_probabilities[key][next_key] = transitions[key][next_key] / topic_counts[key]\n",
    "\n",
    "    # Prepare data for heatmap\n",
    "    users = list(df['user_id'].unique())\n",
    "    months = list(df['month'].unique())\n",
    "    topic_ids = list(set([key[0] for key in conditional_probabilities.keys()] +\n",
    "                         [key[0] for sub_dict in conditional_probabilities.values() for key in sub_dict.keys()]))\n",
    "    topic_ids.sort()\n",
    "\n",
    "    # Create a DataFrame to store the probabilities\n",
    "    heatmap_data = pd.DataFrame(0, index=pd.MultiIndex.from_product([users, topic_ids, topic_ids, months, months],\n",
    "                                                                    names=['User', 'Topic X', 'Topic Y', 'Month X',\n",
    "                                                                           'Month Y']),\n",
    "                                columns=['Probability'])\n",
    "    for (topic_x, month_x, user_id), transitions_dict in conditional_probabilities.items():\n",
    "        for (topic_y, month_y, user_id), prob in transitions_dict.items():\n",
    "            if month_x != month_y:\n",
    "                heatmap_data.loc[\n",
    "                    (user_id, topic_x, topic_y, month_x,\n",
    "                     month_y), 'Probability'] = prob\n",
    "\n",
    "    # Reset the index to prepare for the heatmap\n",
    "\n",
    "    heatmap_data.reset_index(inplace=True)\n",
    "    users = set(heatmap_data['User'].tolist())\n",
    "    for item in users:\n",
    "        #print(item)\n",
    "        #print(heatmap_data[heatmap_data['User'] == item])\n",
    "        heatmap_data_by_user = heatmap_data[heatmap_data['User'] == item]\n",
    "        print(heatmap_data_by_user)\n",
    "        # Create a new column to combine relevant labels for display\n",
    "        heatmap_data_by_user['label'] = heatmap_data_by_user.apply(\n",
    "            lambda row: f\"Prob:{row['Probability']}<br>Mx:{str(row['Month X'])}, My: {str(row['Month Y'])}\", axis=1)\n",
    "        # Create the heatmap\n",
    "        min_marker_size = 5\n",
    "        # Create Scatter plot with Plotly Graph Objects\n",
    "        fig = go.Figure()\n",
    "        fig.add_trace(go.Scatter(\n",
    "            #heatmap_data_by_user,\n",
    "            x=heatmap_data_by_user['Topic X'],\n",
    "            y=heatmap_data_by_user['Topic Y'],\n",
    "            text=heatmap_data_by_user['label'],\n",
    "            # size='Probability',\n",
    "            #color_continuous_scale='Viridis',\n",
    "            #title=f'Conditional Probability of Topic Transitions by User {item}',\n",
    "            # labels={'topic_x': 'Topic X', 'topic_y': 'Topic Y', 'month_x': 'Month X', 'month_y': 'Month Y'},\n",
    "            texttemplate='%{text}',\n",
    "            mode='markers+text',\n",
    "            name='Markers'\n",
    "        )\n",
    "        )\n",
    "        # Set marker size range\n",
    "\n",
    "        # Update traces to ensure minimum marker size\n",
    "        #fig.update_traces(marker=dict(sizemin=10))\n",
    "\n",
    "        # Ensure text is displayed on markers\n",
    "        fig.update_traces(textposition='top center', textfont_size=10)\n",
    "\n",
    "        # Update the layout\n",
    "        fig.update_xaxes(tickmode='linear', dtick=1)\n",
    "        fig.update_yaxes(tickmode='linear', dtick=1)\n",
    "        fig.update_layout(\n",
    "            xaxis_title='Topic X (Month 1)',\n",
    "            yaxis_title='Topic Y (Month 2)'\n",
    "        )\n",
    "        fig.update_layout(title=f'Conditional Probability of Topic Transitions by User {item}')\n",
    "\n",
    "        # Show the figure\n",
    "        fig.write_html(f'conditional_probability_{item}.html')\n",
    "        #fig.show()\n",
    "\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 42
  },
  {
   "cell_type": "code",
   "id": "3210eadd371c04b2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T21:25:33.704119Z",
     "start_time": "2024-07-16T21:25:33.663376Z"
    }
   },
   "source": [
    "from itertools import zip_longest\n",
    "\n",
    "\n",
    "def visualize_outgoing_relation_using_plotly():\n",
    "    time_intervals_tweets = pd.date_range(start=tweets_df['date'].min(), end=tweets_df['date'].max(), freq='M')\n",
    "    result_df_outgoing = pd.DataFrame(columns=['date', 'result'])\n",
    "    result_df_incoming = pd.DataFrame(columns=['date', 'result'])\n",
    "    for time_interval in time_intervals_tweets:\n",
    "        # Filter data for the current time interval\n",
    "        data_interval = tweets_df[\n",
    "            (tweets_df['date'] >= time_interval) & (tweets_df['date'] < time_interval + pd.DateOffset(months=1))]\n",
    "        result_outgoing, result_incoming = find_topics_by_tweets(data_interval)\n",
    "        result_df_outgoing.loc[len(result_df_outgoing)] = {'date': time_interval,\n",
    "                                                           'result': result_outgoing.to_dict('records')}\n",
    "        result_df_incoming.loc[len(result_df_incoming)] = {'date': time_interval,\n",
    "                                                           'result': result_incoming.to_dict('records')}\n",
    "    result_outgoing = []\n",
    "    result_incoming = []\n",
    "    for topic_id in set(tweets_df['topics'].tolist()):\n",
    "        dates = []\n",
    "        relations = []\n",
    "        for index, row in result_df_outgoing.iterrows():\n",
    "            result = row['result']\n",
    "            date = row['date']\n",
    "            result_temp = []\n",
    "            for item in result:\n",
    "                source_topic = item['source']\n",
    "                if source_topic == topic_id:\n",
    "                    dates.append(date)\n",
    "                    for derived_topics_item in item['target']:\n",
    "                        weight = derived_topics_item['count']\n",
    "                        target_topic = derived_topics_item['target_topic']\n",
    "                        result_temp.append({'weight': weight, 'target_topic': target_topic})\n",
    "            relations.append({'date': str(date), 'relations': result_temp})\n",
    "        result_outgoing.append({'source_topic': topic_id, 'target': relations})\n",
    "    for topic_id in set(replies_df['topics'].tolist() + quotes_df['topics'].tolist()):\n",
    "        dates = []\n",
    "        relations = []\n",
    "        for index, row in result_df_incoming.iterrows():\n",
    "            result = row['result']\n",
    "            date = row['date']\n",
    "            result_temp = []\n",
    "            for item in result:\n",
    "                source_topic = item['source']\n",
    "                if source_topic == topic_id:\n",
    "                    dates.append(date)\n",
    "                    for derived_topics_item in item['target']:\n",
    "                        weight = derived_topics_item['count']\n",
    "                        target_topic = derived_topics_item['target_topic']\n",
    "                        result_temp.append({'weight': weight, 'target_topic': target_topic})\n",
    "            relations.append({'date': str(date), 'relations': result_temp})\n",
    "        result_incoming.append({'source_topic': topic_id, 'target': relations})\n",
    "\n",
    "    print(result_incoming)\n",
    "\n",
    "    for item_outgoing, item_incoming in zip(result_outgoing, result_incoming):\n",
    "        df_plotly = pd.DataFrame(columns=['Date', 'Derived Topic', 'Weight'])\n",
    "\n",
    "        source = item_outgoing['source_topic']\n",
    "        target = item_outgoing['target']\n",
    "        for item_target in target:\n",
    "            date = item_target['date']\n",
    "            relations = item_target['relations']\n",
    "            for item_relation in relations:\n",
    "                target_topic = item_relation['target_topic']\n",
    "                weight = item_relation['weight']\n",
    "                df_plotly.loc[len(df_plotly)] = {'Date': date, 'Derived Topic': target_topic, 'Weight': weight}\n",
    "\n",
    "        df_plotly_incoming = pd.DataFrame(columns=['Date', 'Derived Topic', 'Weight'])\n",
    "\n",
    "        source_incoming = item_incoming['source_topic']\n",
    "        target = item_incoming['target']\n",
    "        for item_target in target:\n",
    "            date = item_target['date']\n",
    "            relations = item_target['relations']\n",
    "            for item_relation in relations:\n",
    "                target_topic = item_relation['target_topic']\n",
    "                weight = item_relation['weight']\n",
    "                df_plotly_incoming.loc[len(df_plotly_incoming)] = {'Date': date, 'Derived Topic': target_topic,\n",
    "                                                                   'Weight': weight}\n",
    "\n",
    "        df_plotly['Relation direction'] = 'Outgoing_Tweets->Replies_Quotes'\n",
    "        df_plotly_incoming['Relation direction'] = 'Incoming_Replies_Quotes->Tweets'\n",
    "\n",
    "        # Combine the dataframes\n",
    "        df_combined = pd.concat([df_plotly, df_plotly_incoming])\n",
    "\n",
    "        # Create the bubble chart\n",
    "        fig = px.scatter(df_combined, x='Date', y='Derived Topic', size='Weight', color='Relation direction',\n",
    "                         title=f'Main Topic {source}')\n",
    "\n",
    "        # Show the plot\n",
    "        fig.write_html(f'Main Topic {source}.html')\n",
    "        fig.show()\n",
    "\n",
    "        #fig.write_html(f'Main Topic {source}.html')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 43
  },
  {
   "cell_type": "markdown",
   "id": "aee825ac712b5aea",
   "metadata": {},
   "source": [
    "# Building Network Analysis for both Directions"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T21:25:33.868235Z",
     "start_time": "2024-07-16T21:25:33.820036Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "import random\n",
    "\n",
    "\n",
    "def build_network_analysis():\n",
    "    net = Network(height='800px', width='100%', bgcolor='#222222', font_color='white', )\n",
    "    # Customize node appearance\n",
    "    node_color = 'skyblue'\n",
    "    node_size = 15\n",
    "    font_color = 'black'\n",
    "    font_size = 20\n",
    "    font_face = 'arial'\n",
    "    outgoing_df, incoming_df = find_topics_by_tweets(tweets_df)\n",
    "    # Create a Graph\n",
    "    g_outgoing = nx.Graph()\n",
    "    #g_outgoing.add_edges_from(tweets_df['topics'])\n",
    "    for index, row in outgoing_df.iterrows():\n",
    "        source = row['source']\n",
    "        g_outgoing.add_node(source, label=f'Topic {source}')\n",
    "        for item in row['target']:\n",
    "            weight = item['count']\n",
    "            target = item['target_topic']\n",
    "            g_outgoing.add_edge(source, target, weight=weight * 100)\n",
    "    #label=f'Edge {source}-{target}')\n",
    "    pagerank_threshold = 0.015\n",
    "    # Compute PageRank\n",
    "    pagerank = nx.pagerank(g_outgoing)\n",
    "    # Filter nodes with PageRank above threshold\n",
    "    #high_pagerank_nodes = {node for node, score in pagerank.items() if score >= pagerank_threshold}\n",
    "    # Step 4: Filter nodes based on the threshold\n",
    "    filtered_nodes = [node for node, rank in pagerank.items() if rank >= pagerank_threshold]\n",
    "    filtered_edges = [(u, v) for u, v in g_outgoing.edges() if u in filtered_nodes and v in filtered_nodes]\n",
    "\n",
    "    # Create a new graph with filtered nodes and edges\n",
    "    filtered_G = nx.Graph()\n",
    "    filtered_G.add_nodes_from(filtered_nodes)\n",
    "    filtered_G.add_edges_from(filtered_edges)\n",
    "    nt.barnes_hut()\n",
    "    for node in filtered_G.nodes:\n",
    "        nt.add_node(node, label=f'Topic {node}', color=node_color, size=node_size,\n",
    "                    font={'color': font_color, 'size': font_size, 'face': font_face})\n",
    "    for edge in filtered_G.edges:\n",
    "        nt.add_edge(edge[0], edge[1])\n",
    "    #nt.node_font_size = 20\n",
    "    #nt.barnes_hut()\n",
    "    nt.repulsion(node_distance=400, central_gravity=0.6, spring_length=300, spring_strength=0.05, damping=0.09)\n",
    "    nt.show(\"network_analysis_graph_outgoing_0.015.html\", notebook=False)\n",
    "    #nt.repulsion()\n",
    "    # Compute the best partition\n",
    "    partition = community_louvain.best_partition(filtered_G)\n",
    "    print(\"Communities length :\", len(partition.items()))\n",
    "    # Assign a unique color to each community\n",
    "    community_colors = {}\n",
    "    for node, comm in partition.items():\n",
    "        if comm not in community_colors:\n",
    "            community_colors[comm] = \"#{:06x}\".format(random.randint(0, 0xFFFFFF))\n",
    "\n",
    "    # Add nodes and edges to the PyVis network with community colors\n",
    "    for node in filtered_G.nodes():\n",
    "        nt1.add_node(node, color=community_colors[partition[node]], label=f'Topic {node}', size=node_size,\n",
    "                     font={'color': font_color, 'size': font_size, 'face': font_face})\n",
    "    for edge in filtered_G.edges():\n",
    "        nt1.add_edge(edge[0], edge[1])\n",
    "\n",
    "    # Save the network as an HTML file\n",
    "    nt1.repulsion(node_distance=400, central_gravity=0.6, spring_length=300, spring_strength=0.05, damping=0.09)\n",
    "    nt1.show(\"network_analysis_graph_outgoing_with_community_detection_0.015.html\", notebook=False)\n",
    "\n",
    "    # Create a Graph Incoming\n",
    "    g_incoming = nx.Graph()\n",
    "    for index, row in incoming_df.iterrows():\n",
    "        source = row['source']\n",
    "        g_incoming.add_node(source, label=f'Topic {source}')\n",
    "        for item in row['target']:\n",
    "            weight = item['count']\n",
    "            target = item['target_topic']\n",
    "            g_incoming.add_edge(source, target, weight=weight * 1000)\n",
    "\n",
    "    pagerank = nx.pagerank(g_incoming)\n",
    "    filtered_nodes = [node for node, rank in pagerank.items() if rank >= pagerank_threshold]\n",
    "    filtered_edges = [(u, v) for u, v in g_incoming.edges() if u in filtered_nodes and v in filtered_nodes]\n",
    "\n",
    "    # Create a new graph with filtered nodes and edges\n",
    "    filtered_G = nx.Graph()\n",
    "    filtered_G.add_nodes_from(filtered_nodes)\n",
    "    filtered_G.add_edges_from(filtered_edges)\n",
    "    for node in filtered_G.nodes:\n",
    "        nt2.add_node(node, label=f'Topic {node}', color=node_color, size=node_size,\n",
    "                     font={'color': font_color, 'size': font_size, 'face': font_face})\n",
    "    for edge in filtered_G.edges:\n",
    "        nt2.add_edge(edge[0], edge[1])\n",
    "    #nt.node_font_size = 20\n",
    "    nt2.repulsion(node_distance=400, central_gravity=0.6, spring_length=300, spring_strength=0.05, damping=0.09)\n",
    "    nt2.show(\"network_analysis_graph_incoming_0.015.html\", notebook=False)\n",
    "\n",
    "    # Compute the best partition\n",
    "    partition = community_louvain.best_partition(filtered_G)\n",
    "\n",
    "    community_colors = {}\n",
    "    for node, comm in partition.items():\n",
    "        if comm not in community_colors:\n",
    "            community_colors[comm] = \"#{:06x}\".format(random.randint(0, 0xFFFFFF))\n",
    "\n",
    "    # Add nodes and edges to the PyVis network with community colors\n",
    "    for node in filtered_G.nodes():\n",
    "        nt3.add_node(node, color=community_colors[partition[node]], label=f'Topic {node}', size=node_size,\n",
    "                     font={'color': font_color, 'size': font_size, 'face': font_face})\n",
    "    for edge in filtered_G.edges():\n",
    "        nt3.add_edge(edge[0], edge[1])\n",
    "    nt3.repulsion(node_distance=400, central_gravity=0.6, spring_length=300, spring_strength=0.05, damping=0.09)\n",
    "    # Save the network as an HTML file\n",
    "    nt3.show(\"network_analysis_graph_incoming_with_community_detection_0.015.html\", notebook=False)\n",
    "\n",
    "\n"
   ],
   "id": "913deea6f29d0dfb",
   "outputs": [],
   "execution_count": 44
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Apply Granger Causality between Topics from tweets in both directions: From Topic A to Topic B and From Topic B to Topic A",
   "id": "2a6f2829b96302d7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T21:25:33.943558Z",
     "start_time": "2024-07-16T21:25:33.885310Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from statsmodels.tsa.stattools import grangercausalitytests\n",
    "\n",
    "\n",
    "# Function to perform Granger causality tests\n",
    "def granger_causality_tests(topic_counts: pd.DataFrame, max_lag=3):\n",
    "    topics = topic_counts.columns\n",
    "    results = {}\n",
    "    for topic_a in topics:\n",
    "        for topic_b in topics:\n",
    "            if topic_a != topic_b:\n",
    "                # Test if topic_a Granger-causes topic_b\n",
    "                test_result_ab = grangercausalitytests(topic_counts[[topic_a, topic_b]], max_lag, verbose=False)\n",
    "                p_values_ab = [round(test_result_ab[i + 1][0]['ssr_ftest'][1], 4) for i in range(max_lag)]\n",
    "                results[(topic_a, topic_b, 'A->B')] = p_values_ab\n",
    "\n",
    "                # Test if topic_b Granger-causes topic_a\n",
    "                test_result_ba = grangercausalitytests(topic_counts[[topic_b, topic_a]], max_lag, verbose=False)\n",
    "                p_values_ba = [round(test_result_ba[i + 1][0]['ssr_ftest'][1], 4) for i in range(max_lag)]\n",
    "                results[(topic_b, topic_a, 'B->A')] = p_values_ba\n",
    "    return results\n",
    "\n",
    "\n",
    "def apply_granger_causality_between_topics_from_tweets():\n",
    "    # Add a 'month' column to the DataFrame\n",
    "    tweets_df['month'] = tweets_df['date'].dt.to_period('M')\n",
    "\n",
    "    # Aggregate topic counts by month\n",
    "    topic_counts: pd.DataFrame = tweets_df.groupby(['month', 'topics']).size().unstack(fill_value=0)\n",
    "    # Perform Granger causality tests\n",
    "    granger_results = granger_causality_tests(topic_counts, max_lag=3)\n",
    "\n",
    "    # Print results\n",
    "    for key, value in granger_results.items():\n",
    "        print(f\"Granger causality p-values for {key[2]}: {key[0]} causing {key[1]}: {value}\")\n",
    "\n",
    "        # Filter significant results (e.g., p-value < 0.05)\n",
    "    significant_results = []\n",
    "    for key, value in granger_results.items():\n",
    "        if any(p_val < 0.05 for p_val in value):\n",
    "            significant_results.append((key[0], key[1], key[2], value))\n",
    "\n",
    "    # Create DataFrame for visualization\n",
    "    df_viz = pd.DataFrame(significant_results, columns=['TopicA', 'TopicB', 'Direction', 'P-Values'])\n",
    "\n",
    "    df_viz['Weight'] = df_viz['P-Values'].apply(lambda pvals: 1 - min(pvals))  # Weight based on smallest \n",
    "    # p-value\n",
    "    # Create a directed graph\n",
    "    G = nx.DiGraph()\n",
    "\n",
    "    # Add edges to the graph\n",
    "    for _, row in df_viz.iterrows():\n",
    "        source, target = row['TopicA'], row['TopicB']\n",
    "        direction = row['Direction']\n",
    "        weight = row['Weight']\n",
    "        # Add nodes with different colors based on source/target type\n",
    "        G.add_edge(f\"{source}\", f\"{target}\", weight=weight,\n",
    "                   title=f\"{source}->{target} Weight: {weight:.4f}\")\n",
    "\n",
    "    # Create a pivot table for the heatmap\n",
    "    aggregated_df = df_viz.groupby(['TopicA', 'TopicB'], as_index=False).sum()\n",
    "\n",
    "    heatmap_data = aggregated_df.pivot(index='TopicA', columns='TopicB', values='Weight').fillna(0)\n",
    "\n",
    "    # Create the heatmap\n",
    "    heatmap = go.Heatmap(\n",
    "        z=heatmap_data.values,\n",
    "        x=heatmap_data.columns,\n",
    "        y=heatmap_data.index,\n",
    "        colorscale='Cividis',  # Change the color scale here\n",
    "        text=heatmap_data.values,\n",
    "        hoverinfo='text'\n",
    "    )\n",
    "\n",
    "    # Add annotations\n",
    "    annotations = []\n",
    "    for i, row in enumerate(heatmap_data.index):\n",
    "        for j, col in enumerate(heatmap_data.columns):\n",
    "            weight = heatmap_data.iloc[i, j]\n",
    "            annotations.append(\n",
    "                dict(\n",
    "                    x=col,\n",
    "                    y=row,\n",
    "                    text=str(weight),\n",
    "                    showarrow=False,\n",
    "                    font=dict(color='white' if weight > heatmap.z.max() / 2 else 'black')\n",
    "                )\n",
    "            )\n",
    "\n",
    "    # Create a matrix for the heatmap\n",
    "    topicsA = df_viz['TopicA'].unique()\n",
    "    topicsB = df_viz['TopicB'].unique()\n",
    "    aggregated_df = df_viz.groupby(['TopicA', 'TopicB'], as_index=False).sum()\n",
    "\n",
    "    # Step 2: Create mappings from topics to indices\n",
    "    topics = sorted(set(aggregated_df['TopicA']).union(set(aggregated_df['TopicB'])))\n",
    "    topic_to_index = {topic: i for i, topic in enumerate(topics)}\n",
    "\n",
    "    heatmap_matrix = np.zeros((len(topics), len(topics)))\n",
    "    for _, row in df_viz.iterrows():\n",
    "        source, target = row['TopicA'], row['TopicB']\n",
    "        weight = row['Weight']\n",
    "        source_index = topic_to_index[source]\n",
    "        target_index = topic_to_index[target]\n",
    "        #heatmap_matrix[source_index, target_index] = weight\n",
    "    # Create the heatmap\n",
    "    \"\"\"fig = go.Figure(data=go.Heatmap(\n",
    "        z=heatmap_matrix,\n",
    "        x=topicsA,\n",
    "        y=topicsB,\n",
    "        colorscale='Blues'\n",
    "    ))\n",
    "\n",
    "    fig.update_layout(\n",
    "        title='Granger causality between topics from tweets and replies/quotes',\n",
    "        xaxis_title='TopicB',\n",
    "        yaxis_title='TopicA'\n",
    "    )\n",
    "    \"\"\"\n",
    "    # Step 1: Aggregate the weights for duplicate entries\n",
    "    aggregated_df = df_viz.groupby(['TopicA', 'TopicB', 'Direction'], as_index=False).sum()\n",
    "\n",
    "    # Step 2: Create mappings from topics to indices\n",
    "    topics = sorted(set(aggregated_df['TopicA']).union(set(aggregated_df['TopicB'])))\n",
    "    topic_to_index = {topic: i for i, topic in enumerate(topics)}\n",
    "\n",
    "    # Step 3: Initialize the matrix with zeros\n",
    "    heatmap_matrix = np.zeros((len(topics), len(topics)))\n",
    "\n",
    "    # Step 4: Fill the matrix based on direction and weight\n",
    "    for _, row in aggregated_df.iterrows():\n",
    "        topic_a = row['TopicA']\n",
    "        topic_b = row['TopicB']\n",
    "        direction = row['Direction']\n",
    "        weight = row['Weight']\n",
    "\n",
    "        if direction == 'A->B':\n",
    "            heatmap_matrix[topic_to_index[topic_a], topic_to_index[topic_b]] = weight\n",
    "        elif direction == 'B->A':\n",
    "            heatmap_matrix[topic_to_index[topic_b], topic_to_index[topic_a]] = weight\n",
    "\n",
    "        # Step 5: Plot the heatmap using Plotly\n",
    "    fig = go.Figure(data=go.Heatmap(\n",
    "        z=heatmap_matrix,\n",
    "        x=topics,\n",
    "        y=topics,\n",
    "        colorscale='Viridis',\n",
    "    ))\n",
    "\n",
    "    fig.update_layout(\n",
    "        title='Granger causality between topics from tweets',\n",
    "        xaxis_title='TopicY',\n",
    "        yaxis_title='TopicX',\n",
    "        xaxis=dict(tickmode='array', tickvals=list(range(len(topics))), ticktext=topics),\n",
    "        yaxis=dict(tickmode='array', tickvals=list(range(len(topics))), ticktext=topics),\n",
    "        width=1000,\n",
    "        height=1000\n",
    "    )\n",
    "\n",
    "    fig.show()\n",
    "    fig.write_html(\"granger_causality_between_topics_from_tweets.html\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "id": "a519ec211a00bf68",
   "outputs": [],
   "execution_count": 45
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T21:25:34.066444Z",
     "start_time": "2024-07-16T21:25:34.034525Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Function to perform Granger causality tests between tweets and replies topics\n",
    "def granger_causality_tests_between_sources(tweets_counts, replies_counts, max_lag=3):\n",
    "    results = {}\n",
    "    common_topics = set(tweets_counts.columns).intersection(replies_counts.columns)\n",
    "    for topic in common_topics:\n",
    "        # Align the dataframes to ensure they have the same time index\n",
    "        combined_df = pd.concat([tweets_counts[topic], replies_counts[topic]], axis=1).dropna()\n",
    "        combined_df.columns = ['tweets', 'replies_quotes']\n",
    "\n",
    "        # Test if topic in tweets Granger-causes the same topic in replies\n",
    "        test_result_tr = grangercausalitytests(combined_df[['tweets', 'replies_quotes']], max_lag, verbose=False)\n",
    "        p_values_tr = [round(test_result_tr[i + 1][0]['ssr_ftest'][1], 4) for i in range(max_lag)]\n",
    "        results[(topic, 'Tweets->Replies/Quotes')] = p_values_tr\n",
    "\n",
    "        # Test if topic in replies Granger-causes the same topic in tweets\n",
    "        test_result_rt = grangercausalitytests(combined_df[['replies_quotes', 'tweets']], max_lag, verbose=False)\n",
    "        p_values_rt = [round(test_result_rt[i + 1][0]['ssr_ftest'][1], 4) for i in range(max_lag)]\n",
    "        results[(topic, 'Replies/Quotes->Tweets')] = p_values_rt\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def apply_granger_causality_between_topics_from_tweets_and_replies():\n",
    "    print('ddd')\n",
    "    tweets_df['month'] = tweets_df['date'].dt.to_period('M')\n",
    "    replies_df['month'] = replies_df['date'].dt.to_period('M')\n",
    "    quotes_df['month'] = quotes_df['date'].dt.to_period('M')\n",
    "    # Aggregate topic counts by month\n",
    "    tweets_topic_counts = tweets_df.groupby(['month', 'topics']).size().unstack(fill_value=0)\n",
    "    replies_quotes_topic_counts = replies_df.groupby(['month', 'topics']).size().unstack(\n",
    "        fill_value=0) + quotes_df.groupby(['month', 'topics']).size().unstack(fill_value=0)\n",
    "    # Perform Granger causality tests\n",
    "    granger_results = granger_causality_tests_between_sources(\n",
    "        tweets_topic_counts,\n",
    "        replies_quotes_topic_counts,\n",
    "        max_lag=3)\n",
    "\n",
    "    # Filter significant results (e.g., p-value < 0.05)\n",
    "    significant_results = []\n",
    "    for key, value in granger_results.items():\n",
    "        if any(p_val < 0.05 for p_val in value):\n",
    "            significant_results.append((key[0], key[1], value))\n",
    "\n",
    "    # Create DataFrame for visualization\n",
    "    df_viz = pd.DataFrame(significant_results, columns=['Topic', 'Direction', 'P-Values'])\n",
    "\n",
    "    df_viz['Weight'] = df_viz['P-Values'].apply(lambda pvals: 1 - min(pvals))  # Weight based on smallest \n",
    "    # p-value\n",
    "    # Create a directed graph\n",
    "    G = nx.DiGraph()\n",
    "\n",
    "    # Add edges to the graph\n",
    "    for _, row in df_viz.iterrows():\n",
    "        topic = row['Topic']\n",
    "        direction = row['Direction']\n",
    "        weight = row['Weight']\n",
    "        source, target = direction.split('->')\n",
    "        source_node = f\"{source}_{topic}\"\n",
    "        target_node = f\"{target}_{topic}\"\n",
    "        # Add nodes with different colors based on source/target type\n",
    "        G.add_edge(f\"{source}_{topic}\", f\"{target}_{topic}\", weight=weight,\n",
    "                   title=f\"{source}->{target} Weight: {weight:.4f}\")\n",
    "    fig = go.Figure(data=[go.Table(\n",
    "        header=dict(values=list(df_viz.columns),\n",
    "                    fill_color='paleturquoise',\n",
    "                    align='left'),\n",
    "        cells=dict(values=[df_viz.Topic, df_viz.Direction, df_viz['P-Values'], df_viz.Weight],\n",
    "                   fill_color='lavender',\n",
    "                   align='left'))\n",
    "    ])\n",
    "    # Create a matrix for the heatmap\n",
    "    topics = df_viz['Topic'].unique()\n",
    "    directions = df_viz['Direction'].unique()\n",
    "\n",
    "    heatmap_matrix = np.zeros((len(topics), len(directions)))\n",
    "    for i, topic in enumerate(topics):\n",
    "        for j, direction in enumerate(directions):\n",
    "            weight = df_viz[(df_viz['Topic'] == topic) & (df_viz['Direction'] == direction)]['Weight'].values\n",
    "            if len(weight) > 0:\n",
    "                heatmap_matrix[i, j] = weight[0]\n",
    "    # Create the heatmap\n",
    "    fig = go.Figure(data=go.Heatmap(\n",
    "        z=heatmap_matrix,\n",
    "        x=directions,\n",
    "        y=topics,\n",
    "        colorscale='Blues'\n",
    "    ))\n",
    "\n",
    "    fig.update_layout(\n",
    "        title='Granger causality between topics from tweets and replies/quotes',\n",
    "        xaxis_title='Direction',\n",
    "        yaxis_title='Topic'\n",
    "    )\n",
    "\n",
    "    fig.show()\n",
    "    fig.write_html(\"granger_causality_between_topics_from_tweets_replies_quotes.html\")"
   ],
   "id": "be75e5a3f2334651",
   "outputs": [],
   "execution_count": 46
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T21:25:34.176107Z",
     "start_time": "2024-07-16T21:25:34.171891Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "68ec52e9a748e812",
   "outputs": [],
   "execution_count": 46
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "c8f4c3f12b187092"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T21:25:34.251296Z",
     "start_time": "2024-07-16T21:25:34.236479Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def visualize_evolution(edge_weights_over_time: pd.DataFrame):\n",
    "    # Plotting\n",
    "\n",
    "    print(\"topics connection with topics from replies/ quotes after normalization\")\n",
    "\n",
    "    for index, row in edge_weights_over_time.iterrows():\n",
    "        dates = []\n",
    "        time_periods = row['date']\n",
    "        dates.append(time_periods)\n",
    "        result = row['result']\n",
    "\n",
    "        topics_set = []\n",
    "        derived_topics_set = []\n",
    "        for item in result:\n",
    "            if item['source'] not in topics_set:\n",
    "                topics_set.append(item['source'])\n",
    "            for derived_topics_item in item['target']:\n",
    "                if derived_topics_item['target_topic'] not in derived_topics_set:\n",
    "                    derived_topics_set.append(derived_topics_item['target_topic'])\n",
    "\n",
    "        topics_set.sort()\n",
    "        derived_topics_set.sort()\n",
    "\n",
    "        plt.figure(num=str(time_periods), figsize=(10, 6))\n",
    "        max_length = len(set(tweets_df['topics'].tolist()))\n",
    "\n",
    "        x_min, x_max = min(topics_set), max(topics_set)\n",
    "        y_min, y_max = min(derived_topics_set), max(derived_topics_set)\n",
    "        grid_shape = (y_max - y_min + 1, x_max - x_min + 1)\n",
    "\n",
    "        # Create a 2D grid for the heatmap\n",
    "        heatmap_data = np.zeros((len(derived_topics_set), len(topics_set)))\n",
    "        for j, item in enumerate(result):\n",
    "            main_topic_id = item['source']\n",
    "            derived_topics = item['target']\n",
    "            for k, derived_topic in enumerate(derived_topics):\n",
    "                heatmap_data[\n",
    "                    list(derived_topics_set).index(derived_topic['target_topic']), list(topics_set).index(\n",
    "                        main_topic_id)] = \\\n",
    "                    derived_topic['count']\n",
    "\n",
    "        ax = sns.heatmap(heatmap_data, cmap='Blues', annot=True, fmt=\".1f\")\n",
    "        ax.set_xticklabels(topics_set)\n",
    "        ax.set_yticklabels(derived_topics_set)\n",
    "\n",
    "        plt.title(f\"Relation between Main topics and derived topics from Replies/Quotes for date {time_periods} \")\n",
    "        plt.xlabel(\"Main Topic\")\n",
    "        plt.ylabel(\"Derived Topic from from Replies/Quotes\")\n",
    "        # Show plot\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "# Display the resulting DataFrame"
   ],
   "id": "f1f2736c5fa4ca3",
   "outputs": [],
   "execution_count": 47
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T21:25:34.429319Z",
     "start_time": "2024-07-16T21:25:34.381081Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def visualize_evolution_test(outgoing: pd.DataFrame, incoming: pd.DataFrame):\n",
    "    # Plotting\n",
    "\n",
    "    print(\"topics connection with topics from replies/ quotes after normalization\")\n",
    "\n",
    "    for index, row in incoming.iterrows():\n",
    "        dates = []\n",
    "        time_periods = row['date']\n",
    "        row_outgoing = outgoing[outgoing['date'] == time_periods]\n",
    "        dates.append(time_periods)\n",
    "        result = row['result']\n",
    "        result_outgoing = row_outgoing['result']\n",
    "\n",
    "        topics_set = []\n",
    "        derived_topics_set = []\n",
    "        topics_set_incoming = []\n",
    "        derived_topics_set_incoming = []\n",
    "        for item in result:\n",
    "            if item['source'] not in topics_set:\n",
    "                topics_set.append(item['source'])\n",
    "            for derived_topics_item in item['target']:\n",
    "                if derived_topics_item['target_topic'] not in derived_topics_set:\n",
    "                    derived_topics_set.append(derived_topics_item['target_topic'])\n",
    "\n",
    "        topics_set_incoming = []\n",
    "        derived_topics_set_incoming = []\n",
    "        for item in result_outgoing:\n",
    "            if item['source'] not in topics_set:\n",
    "                topics_set_incoming.append(item['source'])\n",
    "            for derived_topics_item in item['target']:\n",
    "                if derived_topics_item['target_topic'] not in derived_topics_set:\n",
    "                    derived_topics_set_incoming.append(derived_topics_item['target_topic'])\n",
    "\n",
    "        topics_set.sort()\n",
    "        derived_topics_set.sort()\n",
    "        topics_set_incoming.sort()\n",
    "        derived_topics_set_incoming.sort()\n",
    "\n",
    "        plt.figure(num=str(time_periods), figsize=(10, 6))\n",
    "        max_length = len(set(tweets_df['topics'].tolist()))\n",
    "\n",
    "        x_min, x_max = min(topics_set), max(topics_set)\n",
    "        y_min, y_max = min(derived_topics_set), max(derived_topics_set)\n",
    "        grid_shape = (y_max - y_min + 1, x_max - x_min + 1)\n",
    "\n",
    "        # Create a 2D grid for the heatmap\n",
    "        heatmap_data = np.zeros((len(derived_topics_set), len(topics_set)))\n",
    "        heatmap_data_incoming = np.zeros((len(derived_topics_set_incoming), len(topics_set_incoming)))\n",
    "        for j, item in enumerate(result):\n",
    "            main_topic_id = item['source']\n",
    "            derived_topics = item['target']\n",
    "            for k, derived_topic in enumerate(derived_topics):\n",
    "                heatmap_data[\n",
    "                    list(derived_topics_set).index(derived_topic['target_topic']), list(topics_set).index(\n",
    "                        main_topic_id)] = \\\n",
    "                    derived_topic['count']\n",
    "\n",
    "        for j, item in enumerate(incoming):\n",
    "            main_topic_id = item['source']\n",
    "            derived_topics = item['target']\n",
    "            for k, derived_topic in enumerate(derived_topics):\n",
    "                heatmap_data_incoming[\n",
    "                    list(derived_topics_set).index(derived_topic['target_topic']), list(topics_set).index(\n",
    "                        main_topic_id)] = \\\n",
    "                    derived_topic['count']\n",
    "\n",
    "        ax = sns.heatmap(heatmap_data, cmap='Blues', annot=True, fmt=\".1f\")\n",
    "        sns.heatmap(heatmap_data_incoming, cmap='Reds', annot=True, fmt=\".1f\")\n",
    "        #ax.set_xticklabels(topics_set)\n",
    "        #ax.set_yticklabels(derived_topics_set)\n",
    "\n",
    "        plt.title(f\"Relation between Main topics and derived topics from Replies/Quotes for date {time_periods} \")\n",
    "        plt.xlabel(\"Main Topic\")\n",
    "        plt.ylabel(\"Derived Topic from from Replies/Quotes\")\n",
    "        # Show plot\n",
    "        plt.grid(True)\n",
    "        plt.show()"
   ],
   "id": "dbe68718d3590b6e",
   "outputs": [],
   "execution_count": 48
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T21:25:34.491565Z",
     "start_time": "2024-07-16T21:25:34.470731Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def visualize_evolution_main_topic_overtime(edge_weights_over_time: pd.DataFrame):\n",
    "    # Plotting\n",
    "\n",
    "    print(\"topics connection with topics from replies/ quotes after normalization\")\n",
    "\n",
    "    for index, row in edge_weights_over_time.iterrows():\n",
    "        dates = []\n",
    "        time_periods = row['date']\n",
    "        dates.append(time_periods)\n",
    "        result = row['result']\n",
    "        derived_topics_set = []\n",
    "        weights = []\n",
    "        y_labels = []\n",
    "        for item in result:\n",
    "            print(item)\n",
    "            for derived_topics_item in item['replies_quotes_topics']:\n",
    "                if derived_topics_item['replies_quotes_topic'] not in derived_topics_set:\n",
    "                    derived_topic = derived_topics_item['replies_quotes_topic']\n",
    "                    derived_topics_set.append(derived_topic)\n",
    "                    y_labels.append(f'Derived Topic {derived_topic}')\n",
    "                    weights.append(derived_topics_item['count'])\n",
    "\n",
    "            if len(derived_topics_set) > 0:\n",
    "                #derived_topics_set.sort()\n",
    "                data_array = np.array(derived_topics_set)\n",
    "                data_reshaped = data_array.reshape(-1, 1)\n",
    "                weights_reshaped = np.array(weights).reshape(-1, 1)\n",
    "\n",
    "                plt.figure(num=str(time_periods), figsize=(10, 6))\n",
    "                ax = sns.heatmap(data_reshaped, xticklabels=[], yticklabels=y_labels,\n",
    "                                 annot=weights_reshaped,\n",
    "                                 fmt='.2f', cmap='viridis')\n",
    "                ax.set_xticklabels([])\n",
    "                #ax.set_yticklabels(derived_topics_set)\n",
    "\n",
    "                plt.title(\n",
    "                    f\"Relation between Main topic and derived topics from Replies/Quotes for date {time_periods} \")\n",
    "                plt.xlabel(f\"Main Topic {item['topic']}\")\n",
    "                #plt.ylabel(\"Derived Topic from from Replies/Quotes\")\n",
    "                # Show plot\n",
    "                plt.grid(True)\n",
    "                plt.show()"
   ],
   "id": "2ca0c88165307821",
   "outputs": [],
   "execution_count": 49
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Relation between topics based on user communities",
   "id": "4c5b6d25b8807544"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "570a0d694653973"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T21:25:34.762764Z",
     "start_time": "2024-07-16T21:25:34.592045Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.cluster import KMeans, AffinityPropagation\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from itertools import combinations\n",
    "\n",
    "\n",
    "def test_clustering():\n",
    "    # Example user data at different time points\n",
    "    data_time_points = [\n",
    "        np.array([\n",
    "            [1, 0, 1, 0],\n",
    "            [1, 1, 0, 0],\n",
    "            [0, 1, 1, 1],\n",
    "            [0, 0, 1, 1]\n",
    "        ]),\n",
    "        np.array([\n",
    "            [1, 0, 1, 1],\n",
    "            [1, 1, 1, 0],\n",
    "            [0, 1, 1, 0],\n",
    "            [0, 0, 1, 1]\n",
    "        ]),\n",
    "        np.array([\n",
    "            [1, 1, 0, 1],\n",
    "            [1, 0, 1, 0],\n",
    "            [0, 1, 1, 0],\n",
    "            [0, 0, 1, 0]\n",
    "        ])\n",
    "    ]\n",
    "\n",
    "    k = 2\n",
    "    cluster_results = []\n",
    "\n",
    "    for data in data_time_points:\n",
    "        # Compute similarity matrix\n",
    "        similarity_matrix = cosine_similarity(data)\n",
    "\n",
    "        # Apply k-means clustering\n",
    "        kmeans = KMeans(n_clusters=k, random_state=0).fit(similarity_matrix)\n",
    "        labels = kmeans.labels_\n",
    "\n",
    "        # Store the cluster labels\n",
    "        cluster_results.append(labels)\n",
    "\n",
    "    # Prepare data for Sankey diagram\n",
    "    labels = []\n",
    "    sources = []\n",
    "    targets = []\n",
    "    values = []\n",
    "\n",
    "    # Generate labels for clusters at each time point\n",
    "    for t in range(len(data_time_points)):\n",
    "        for i in range(k):\n",
    "            labels.append(f'Time{t + 1}_Cluster{i}')\n",
    "\n",
    "    # Map clusters to their indices\n",
    "    label_indices = {label: i for i, label in enumerate(labels)}\n",
    "\n",
    "    # Create Sankey diagram connections for each pair of consecutive time points\n",
    "    for t in range(len(data_time_points) - 1):\n",
    "        labels_time1 = cluster_results[t]\n",
    "        labels_time2 = cluster_results[t + 1]\n",
    "        transition_matrix = create_transition_matrix(labels_time1, labels_time2, k)\n",
    "\n",
    "        for i in range(k):\n",
    "            for j in range(k):\n",
    "                if transition_matrix.iloc[i, j] > 0:\n",
    "                    sources.append(label_indices[f'Time{t + 1}_Cluster{i}'])\n",
    "                    targets.append(label_indices[f'Time{t + 2}_Cluster{j}'])\n",
    "                    values.append(transition_matrix.iloc[i, j])\n",
    "\n",
    "    # Create Sankey diagram\n",
    "    fig = go.Figure(data=[go.Sankey(\n",
    "        node=dict(\n",
    "            pad=15,\n",
    "            thickness=20,\n",
    "            line=dict(color=\"black\", width=0.5),\n",
    "            label=labels,\n",
    "        ),\n",
    "        link=dict(\n",
    "            source=sources,\n",
    "            target=targets,\n",
    "            value=values\n",
    "        )\n",
    "    )])\n",
    "\n",
    "    fig.update_layout(title_text=\"User Cluster Transitions Over Time\", font_size=10)\n",
    "    fig.show()\n",
    "\n",
    "\n",
    "# Function to create transition matrix for two consecutive time points\n",
    "def create_transition_matrix(labels_time1, labels_time2):\n",
    "    transition_matrix = pd.crosstab(labels_time1, labels_time2, rownames=['Time1'], colnames=['Time2'])\n",
    "    return transition_matrix\n",
    "\n",
    "\n",
    "def normalize_2d_array(array):\n",
    "    # Find the minimum and maximum values in the array\n",
    "    min_val = np.min(array)\n",
    "    max_val = np.max(array)\n",
    "\n",
    "    # Normalize the array using the min-max formula\n",
    "    normalized_array = (array - min_val) / (max_val - min_val)\n",
    "\n",
    "    print(\"Normalized Array :\", normalized_array)\n",
    "    return normalized_array\n",
    "\n",
    "\n",
    "def find_relations_politicians():\n",
    "    # Get unique user IDs and topics\n",
    "    user_ids = set(tweets_df['user_id'].tolist())\n",
    "    topics = set(tweets_df['topics'].tolist())\n",
    "    user_topic_matrices = {}\n",
    "    topic_index = {topic: i for i, topic in enumerate(topics)}\n",
    "    # Example: Splitting into monthly time windows\n",
    "    tweets_df['date'] = pd.to_datetime(tweets_df['date'])\n",
    "    min_timestamp = tweets_df['date'].min()\n",
    "    max_timestamp = tweets_df['date'].max()\n",
    "    time_windows = pd.date_range(start=min_timestamp, end=max_timestamp, freq='M')\n",
    "\n",
    "    # Build the dictionary\n",
    "    for user_id in user_ids:\n",
    "        # Initialize the 2D array for the user\n",
    "        topic_matrix = np.zeros((len(topics), len(topics)))\n",
    "        user_topic_matrices[user_id] = {}\n",
    "        # Filter tweets for the current user_id\n",
    "        user_tweets = tweets_df[tweets_df['user_id'] == user_id]\n",
    "        # Iterate over each time window\n",
    "        for i, window_end in enumerate(time_windows):\n",
    "            window_start = time_windows[i - 1] if i > 0 else min_timestamp\n",
    "\n",
    "            tweets_window = user_tweets[(user_tweets['date'] >= window_start) & (user_tweets['date'] < window_end)]\n",
    "\n",
    "            # Get the topics per user\n",
    "            topics_per_user = set(tweets_window['topics'])\n",
    "            for tweet_topic in topics_per_user:\n",
    "                tweets = tweets_window[tweets_window['topics'] == tweet_topic]['id']\n",
    "                replies_topics = replies_df[replies_df['in_reply_to_tweet_id'].isin(tweets)]['topics'].tolist()\n",
    "                quotes_topics = quotes_df[quotes_df['quoted_tweet_id'].isin(tweets)]['topics'].tolist()\n",
    "                replies_quotes_topics = replies_topics + quotes_topics\n",
    "\n",
    "                for reply_quote_topic in replies_quotes_topics:\n",
    "                    topic_matrix[topic_index[tweet_topic]][topic_index[reply_quote_topic]] += 1\n",
    "\n",
    "            # Save the 2D array in the dictionary\n",
    "            user_topic_matrices[user_id][window_end] = topic_matrix\n",
    "    initial_clusters = 3\n",
    "    user_cluster_labels = {}\n",
    "    count_time_windows = 0\n",
    "    for date in time_windows:\n",
    "        count_time_windows += 1\n",
    "        print('Count for time windows', count_time_windows, len(time_windows))\n",
    "\n",
    "        similarity_matrix = np.zeros((len(user_ids), len(user_ids)))\n",
    "        for i, id1 in enumerate(user_ids):\n",
    "            for j, id2 in enumerate(user_ids):\n",
    "                matrix1 = user_topic_matrices[id1][date]\n",
    "                matrix2 = user_topic_matrices[id2][date]\n",
    "                similarity_result = calculate_similarity(matrix1, matrix2)\n",
    "                similarity_matrix[i][j] = normalize_2d_array(similarity_result)\n",
    "        kmeans_users = KMeans(n_clusters=initial_clusters, random_state=0)\n",
    "        kmeans_users.fit(1 - similarity_matrix)\n",
    "        print('Similarity Matrix :', similarity_matrix)\n",
    "        user_cluster_labels[date] = kmeans_users.labels_\n",
    "        # Prepare data for Sankey diagram\n",
    "    labels = []\n",
    "    sources = []\n",
    "    targets = []\n",
    "    values = []\n",
    "\n",
    "    # Generate labels for clusters at each time point\n",
    "    for t in range(len(time_windows)):\n",
    "        for i in range(initial_clusters):\n",
    "            labels.append(f'Time{t + 1}_Cluster{i}')\n",
    "\n",
    "    # Map clusters to their indices\n",
    "    label_indices = {label: i for i, label in enumerate(labels)}\n",
    "\n",
    "    # Create Sankey diagram connections for each pair of consecutive time points\n",
    "    for t in range(len(time_windows) - 1):\n",
    "        previous_date = time_windows[t - 1]\n",
    "        current_date = time_windows[t]\n",
    "        labels_time1 = user_cluster_labels[previous_date]\n",
    "        labels_time2 = user_cluster_labels[current_date]\n",
    "        transition_matrix = create_transition_matrix(labels_time1, labels_time2)\n",
    "\n",
    "        for i in range(initial_clusters):\n",
    "            for j in range(initial_clusters):\n",
    "                if transition_matrix.iloc[i, j] > 0:\n",
    "                    sources.append(label_indices[f'Time{t + 1}_Cluster{i}'])\n",
    "                    targets.append(label_indices[f'Time{t + 2}_Cluster{j}'])\n",
    "                    values.append(transition_matrix.iloc[i, j])\n",
    "\n",
    "    # Create Sankey diagram\n",
    "    fig = go.Figure(data=[go.Sankey(\n",
    "        node=dict(\n",
    "            pad=15,\n",
    "            thickness=20,\n",
    "            line=dict(color=\"black\", width=0.5),\n",
    "            label=labels,\n",
    "        ),\n",
    "        link=dict(\n",
    "            source=sources,\n",
    "            target=targets,\n",
    "            value=values\n",
    "        )\n",
    "    )])\n",
    "\n",
    "    fig.update_layout(title_text=\"User Cluster Transitions Over Time\", font_size=10)\n",
    "    fig.show()\n",
    "\n",
    "\n",
    "def find_relation_between_topics_based_on_user_communities():\n",
    "    # Get unique user IDs and topics\n",
    "    user_ids = set(tweets_df['user_id'].tolist())\n",
    "    topics = set(tweets_df['topics'].tolist())\n",
    "    print('user ids length : ', len(user_ids))\n",
    "    # Initialize the dictionary\n",
    "    user_topic_matrices = {}\n",
    "    topic_index = {topic: i for i, topic in enumerate(topics)}\n",
    "    # Example: Splitting into monthly time windows\n",
    "    tweets_df['date'] = pd.to_datetime(tweets_df['date'])\n",
    "\n",
    "    # Group tweets by month\n",
    "    tweets_df['month'] = tweets_df['date'].dt.to_period('M')\n",
    "    monthly_groups = tweets_df.groupby('month')\n",
    "\n",
    "    time_windows = monthly_groups\n",
    "\n",
    "    # Build the dictionary\n",
    "    for user_id in user_ids:\n",
    "        # Initialize the 2D array for the user\n",
    "\n",
    "        user_topic_matrices[user_id] = {}\n",
    "        # Filter tweets for the current user_id\n",
    "        #user_tweets = tweets_df[tweets_df['user_id'] == user_id]\n",
    "        # Iterate over each time window\n",
    "        for month, group in monthly_groups:\n",
    "            # Get the topics per user\n",
    "            topic_matrix = np.zeros((len(topics), len(topics)))\n",
    "            group_by_user = group[group['user_id'] == user_id]\n",
    "            topics_per_user = set(group_by_user['topics'])\n",
    "\n",
    "            for tweet_topic in topics_per_user:\n",
    "                tweets = group_by_user[group_by_user['topics'] == tweet_topic]['id'].tolist()\n",
    "                replies_topics = replies_df[replies_df['in_reply_to_tweet_id'].isin(tweets)]['topics'].tolist()\n",
    "                quotes_topics = quotes_df[quotes_df['quoted_tweet_id'].isin(tweets)]['topics'].tolist()\n",
    "                replies_quotes_topics = replies_topics + quotes_topics\n",
    "\n",
    "                for reply_quote_topic in replies_quotes_topics:\n",
    "                    topic_matrix[topic_index[tweet_topic]][topic_index[reply_quote_topic]] += 1\n",
    "\n",
    "            # Save the 2D array in the dictionary\n",
    "            user_topic_matrices[user_id][month] = topic_matrix\n",
    "\n",
    "    print(\"calculate similarity :\")\n",
    "\n",
    "    #print(calculate_jaccard_similarity_between_users(user_topic_matrix))\n",
    "    # Step 2: Perform clustering on users for each time window\n",
    "    user_cluster_labels = {}\n",
    "    count_time_windows = 0\n",
    "    for date, _ in time_windows:\n",
    "        count_time_windows += 1\n",
    "        print('Count for time windows', count_time_windows)\n",
    "        similarity_matrix = np.zeros((len(user_ids), len(user_ids)))\n",
    "        user_id_list = list(user_ids)\n",
    "        # Generate combinations of user IDs\n",
    "        user_combinations = combinations(user_id_list, 2)\n",
    "\n",
    "        # Calculate Jaccard similarity for each pair of users\n",
    "        for id1, id2 in user_combinations:\n",
    "            matrix1 = user_topic_matrices[id1][date]\n",
    "            matrix2 = user_topic_matrices[id2][date]\n",
    "            i = user_id_list.index(id1)\n",
    "            j = user_id_list.index(id2)\n",
    "            #print(f\"User_id  {id1} with matrix {matrix1}\")\n",
    "            #print(f\"User_id  {id2} with matrix {matrix2}\")\n",
    "            similarity_result = calculate_similarity(matrix1, matrix2)\n",
    "            similarity_matrix[i][j] = similarity_result\n",
    "        normalized_similarity_matrix = normalize_2d_array(similarity_matrix)\n",
    "\n",
    "        affinity_propagation = AffinityPropagation(affinity='precomputed', random_state=0)\n",
    "        affinity_propagation.fit(normalized_similarity_matrix)\n",
    "\n",
    "        # Extracting labels (clusters)\n",
    "        user_cluster_labels[date] = affinity_propagation.labels_\n",
    "\n",
    "    # Step 3: Track cluster transitions over time\n",
    "    sankey_data = []\n",
    "    label_prefix = \"Cluster\"\n",
    "    labels = []\n",
    "    source = []\n",
    "    target = []\n",
    "    value = []\n",
    "\n",
    "    # Collect all labels first\n",
    "    max_cluster_id = max([max(labels) for labels in user_cluster_labels.values()])\n",
    "    for i in range(max_cluster_id + 1):\n",
    "        labels.append(f\"{label_prefix} {i}\")\n",
    "\n",
    "    # Collect transitions\n",
    "    group_keys = list(time_windows.groups.keys())  # Convert dict_keys to a list\n",
    "\n",
    "    for i in range(1, len(group_keys)):\n",
    "        previous_date = group_keys[i - 1]\n",
    "        current_date = group_keys[i]\n",
    "        previous_clusters = user_cluster_labels[previous_date]\n",
    "        current_clusters = user_cluster_labels[current_date]\n",
    "        transition_counts = np.zeros((max_cluster_id + 1, max_cluster_id + 1))\n",
    "        for user_index in range(len(user_ids)):\n",
    "            previous_cluster = previous_clusters[user_index]\n",
    "            current_cluster = current_clusters[user_index]\n",
    "            transition_counts[previous_cluster][current_cluster] += 1\n",
    "        for j in range(max_cluster_id + 1):\n",
    "            for k in range(max_cluster_id + 1):\n",
    "                if transition_counts[j][k] > 0:\n",
    "                    source.append((i - 1) * (max_cluster_id + 1) + j)\n",
    "                    target.append(i * (max_cluster_id + 1) + k)\n",
    "                    value.append(transition_counts[j][k])\n",
    "\n",
    "    # Repeat labels for each time window to match the number of nodes in the diagram\n",
    "    all_labels = labels * len(time_windows)\n",
    "\n",
    "    # Step 4: Create Sankey diagram\n",
    "    fig = go.Figure(go.Sankey(\n",
    "        node=dict(\n",
    "            pad=15,\n",
    "            thickness=20,\n",
    "            line=dict(color=\"blue\", width=0.5),\n",
    "            label=all_labels\n",
    "        ),\n",
    "        link=dict(\n",
    "            source=source,\n",
    "            target=target,\n",
    "            value=value\n",
    "        )\n",
    "    ))\n",
    "\n",
    "    fig.update_layout(title_text=\"Sankey Diagram of Politician Transitions between Clusters Over Time\", font_size=10)\n",
    "    fig.show()\n",
    "    fig.write_html(\"Politician_Clustering_Over_Time.html\")\n",
    "\n",
    "\n",
    "def find_relation_between_topics_based_on_user_communities_temp():\n",
    "    # Get unique user IDs and topics\n",
    "    user_ids = set(list(set(tweets_df['user_id'].tolist()))[0:10])\n",
    "    topics = set(tweets_df['topics'].tolist())\n",
    "\n",
    "    # Initialize the dictionary\n",
    "    user_topic_matrices = {}\n",
    "    topic_index = {topic: i for i, topic in enumerate(topics)}\n",
    "    # Example: Splitting into monthly time windows\n",
    "    tweets_df['date'] = pd.to_datetime(tweets_df['date'])\n",
    "    min_timestamp = tweets_df['date'].min()\n",
    "    max_timestamp = tweets_df['date'].max()\n",
    "    time_windows = pd.date_range(start=min_timestamp, end=max_timestamp, freq='M')\n",
    "\n",
    "    # Build the dictionary\n",
    "    for user_id in user_ids:\n",
    "        # Initialize the 2D array for the user\n",
    "        topic_matrix = np.zeros((len(topics), len(topics)))\n",
    "        user_topic_matrices[user_id] = {}\n",
    "        # Iterate over each time window\n",
    "        for i, window_end in enumerate(time_windows):\n",
    "            window_start = time_windows[i - 1] if i > 0 else min_timestamp\n",
    "\n",
    "            # Get the topics per user\n",
    "            topics_per_user = set(tweets_df[(tweets_df['user_id'] == user_id) &\n",
    "                                            (tweets_df['date'] >= window_start) &\n",
    "                                            (tweets_df['date'] <= window_end)]['topics'].tolist())\n",
    "            for tweet_topic in topics_per_user:\n",
    "                tweets = tweets_df[(tweets_df['user_id'] == user_id) & (tweets_df['topics'] == tweet_topic)][\n",
    "                    'id'].tolist()\n",
    "                replies_topics = replies_df[replies_df['in_reply_to_tweet_id'].isin(tweets)]['topics'].tolist()\n",
    "                quotes_topics = quotes_df[quotes_df['quoted_tweet_id'].isin(tweets)]['topics'].tolist()\n",
    "                replies_quotes_topics = replies_topics + quotes_topics\n",
    "\n",
    "                for reply_quote_topic in replies_quotes_topics:\n",
    "                    topic_matrix[topic_index[tweet_topic]][topic_index[reply_quote_topic]] += 1\n",
    "\n",
    "            # Save the 2D array in the dictionary\n",
    "            user_topic_matrices[user_id][window_end] = topic_matrix\n",
    "    # Save topic_index to keep track of the mapping\n",
    "    topic_mapping = {i: topic for topic, i in topic_index.items()}\n",
    "    print(\"calculate similarity :\")\n",
    "    #print(calculate_jaccard_similarity_between_users(user_topic_matrix))\n",
    "    # Get the list of politician IDs\n",
    "    politician_ids = list(user_topic_matrices.keys())\n",
    "\n",
    "    print(\"All Dates iteration :\", len(user_topic_matrices[politician_ids[0]].keys()))\n",
    "\n",
    "    # Initialize Sankey diagram data structures\n",
    "    sankey_data = []\n",
    "\n",
    "    count = 0\n",
    "\n",
    "    # Iterate over each time period\n",
    "    for date in user_topic_matrices[politician_ids[0]].keys():  # Assuming all politicians have the same time periods\n",
    "        # Initialize similarity matrix for the current time period\n",
    "        num_politicians = len(politician_ids)\n",
    "        similarity_matrix = np.zeros((num_politicians, num_politicians))\n",
    "        count += 1\n",
    "        print('Counter for Date iteration ', count)\n",
    "\n",
    "        # Calculate similarity matrix for the current time period\n",
    "        for i, id1 in enumerate(politician_ids):\n",
    "            for j, id2 in enumerate(politician_ids):\n",
    "                matrix1 = user_topic_matrices[id1][date]\n",
    "                matrix2 = user_topic_matrices[id2][date]\n",
    "                similarity_matrix[i][j] = calculate_similarity(matrix1, matrix2)\n",
    "\n",
    "        # Perform clustering (e.g., K-Means)\n",
    "        num_clusters_politicians = 6\n",
    "        kmeans_politicians = KMeans(n_clusters=num_clusters_politicians, random_state=0)\n",
    "        kmeans_politicians.fit(1 - similarity_matrix)\n",
    "\n",
    "        politician_clusters = kmeans_politicians.labels_\n",
    "\n",
    "        # Prepare data for Sankey diagram for the current time period\n",
    "        labels = politician_ids + [f\"Cluster {i}\" for i in range(num_clusters_politicians)]\n",
    "        source = []\n",
    "        target = []\n",
    "        value = []\n",
    "\n",
    "        print(\"Politicians Clusters : \", politician_clusters)\n",
    "\n",
    "        # Links from politicians to their clusters\n",
    "        for i, cluster in enumerate(politician_clusters):\n",
    "            source.append(cluster)\n",
    "            target.append(len(politician_ids) + cluster)\n",
    "            value.append(1)\n",
    "\n",
    "        # Store Sankey diagram data for the current time period\n",
    "        sankey_data.append((labels, source, target, value, date))\n",
    "\n",
    "    # Create animated Sankey diagram\n",
    "    fig = go.Figure()\n",
    "\n",
    "    for labels, source, target, value, date in sankey_data:\n",
    "        fig.add_trace(go.Sankey(\n",
    "            node=dict(\n",
    "                pad=15,\n",
    "                thickness=20,\n",
    "                line=dict(color=\"blue\", width=0.5),\n",
    "                label=labels\n",
    "            ),\n",
    "            link=dict(\n",
    "                source=source,\n",
    "                target=target,\n",
    "                value=value,\n",
    "                color=[\"midnightblue\", \"lightskyblue\", \"gold\", \"mediumturquoise\", \"lightgreen\", \"cyan\"],\n",
    "\n",
    "            ),\n",
    "            name=str(date)  # Use date as the name for each frame\n",
    "        ))\n",
    "\n",
    "    fig.update_layout(title_text=\"Politician Clustering Over Time\", font_size=10)\n",
    "    fig.show()\n",
    "    fig.write_html(\"Politician_Clustering_Over_Time.html\")\n",
    "\n",
    "\n",
    "def is_zero_matrix(matrix):\n",
    "    np_matrix = np.array(matrix)\n",
    "    return np.all(np_matrix == 0)\n",
    "\n",
    "\n",
    "def calculate_similarity_temp(matrix1, matrix2):\n",
    "    num_topics = matrix1.shape[0]\n",
    "    similarities = []\n",
    "\n",
    "    for i in range(num_topics):\n",
    "        vector1 = matrix1[i, :]\n",
    "        vector2 = matrix2[i, :]\n",
    "\n",
    "        # Calculate cosine similarity between the topic vectors\n",
    "        similarity = cosine_similarity([vector1], [vector2])[0][0]\n",
    "        similarities.append(similarity)\n",
    "\n",
    "    # Return the average similarity across all topics\n",
    "    return np.mean(similarities)\n",
    "\n",
    "\n",
    "def calculate_similarity(matrix1, matrix2):\n",
    "    if matrix1.shape != matrix2.shape:\n",
    "        raise ValueError(\"The two matrices must have the same shape.\")\n",
    "    cos_sim_matrix = cosine_similarity(matrix1.T, matrix2.T)\n",
    "    return np.mean(cos_sim_matrix)\n",
    "\n",
    "\n",
    "# Function to calculate Jaccard similarity between two matrices\n",
    "def calculate_jaccard_similarity(matrix1, matrix2):\n",
    "    # Flatten the matrices\n",
    "    vector1 = matrix1.flatten()\n",
    "    vector2 = matrix2.flatten()\n",
    "\n",
    "    # Calculate Jaccard similarity\n",
    "    intersection = len(set(vector1) & set(vector2))\n",
    "    union = len(set(vector1) | set(vector2))\n",
    "    similarity = intersection / union\n",
    "    return similarity\n",
    "\n",
    "\n",
    "# Function to calculate Jaccard similarity between users\n",
    "def calculate_jaccard_similarity_between_users(user_topic_matrices):\n",
    "    similarities = {}\n",
    "    user_ids = list(user_topic_matrices.keys())\n",
    "\n",
    "    # Generate combinations of user IDs\n",
    "    user_combinations = combinations(user_ids, 2)\n",
    "\n",
    "    # Calculate Jaccard similarity for each pair of users\n",
    "    for user1_id, user2_id in user_combinations:\n",
    "        similarity = calculate_jaccard_similarity(user_topic_matrices[user1_id], user_topic_matrices[user2_id])\n",
    "        similarities[(user1_id, user2_id)] = similarity\n",
    "\n",
    "    return similarities\n",
    "\n",
    "\n",
    "\n"
   ],
   "id": "da0bbbe2d68370ae",
   "outputs": [],
   "execution_count": 50
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Tweets Counter Graph over time (monthly)",
   "id": "7f6bf1566bd01a81"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T21:25:34.789036Z",
     "start_time": "2024-07-16T21:25:34.766813Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def visualize_tweets_counter_over_time():\n",
    "    # Convert 'date' column to datetime\n",
    "    tweets_df['date'] = pd.to_datetime(tweets_df['date'])\n",
    "\n",
    "    # Extract year and month\n",
    "    tweets_df['year_month'] = tweets_df['date'].dt.to_period('M')\n",
    "\n",
    "    # Count tweets per month\n",
    "    monthly_tweet_counts = tweets_df.groupby('year_month').size()\n",
    "\n",
    "    # Plot the data\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    monthly_tweet_counts.plot(kind='bar')\n",
    "    plt.title('Number of Tweets per Month')\n",
    "    plt.xlabel('Month')\n",
    "    plt.ylabel('Number of Tweets')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def visualize_replies_counter_over_time():\n",
    "    # Convert 'date' column to datetime\n",
    "    replies_df['date'] = pd.to_datetime(replies_df['date'])\n",
    "\n",
    "    # Extract year and month\n",
    "    replies_df['year_month'] = replies_df['date'].dt.to_period('M')\n",
    "\n",
    "    # Count replies per month\n",
    "    monthly_reply_counts = replies_df.groupby('year_month').size()\n",
    "\n",
    "    # Plot the data\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    monthly_reply_counts.plot(kind='bar')\n",
    "    plt.title('Number of Replies per Month')\n",
    "    plt.xlabel('Month')\n",
    "    plt.ylabel('Number of Replies')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def visualize_quotes_counter_over_time():\n",
    "    # Convert 'date' column to datetime\n",
    "    quotes_df['date'] = pd.to_datetime(quotes_df['date'])\n",
    "\n",
    "    # Extract year and month\n",
    "    quotes_df['year_month'] = quotes_df['date'].dt.to_period('M')\n",
    "\n",
    "    # Count quotes per month\n",
    "    monthly_quote_counts = quotes_df.groupby('year_month').size()\n",
    "\n",
    "    # Plot the data\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    monthly_quote_counts.plot(kind='bar')\n",
    "    plt.title('Number of Quotes per Month')\n",
    "    plt.xlabel('Month')\n",
    "    plt.ylabel('Number of Quotes')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()"
   ],
   "id": "7c254ca7d1803634",
   "outputs": [],
   "execution_count": 51
  },
  {
   "cell_type": "code",
   "id": "6f3f17fcf9d9563a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T21:25:34.822966Z",
     "start_time": "2024-07-16T21:25:34.802047Z"
    }
   },
   "source": [
    "from scipy.stats import mode\n",
    "\n",
    "\n",
    "def track_topic_clusters():\n",
    "    tweets_df['date'] = pd.to_datetime(tweets_df['date'])\n",
    "\n",
    "    # Group tweets by month\n",
    "    tweets_df['month'] = tweets_df['date'].dt.to_period('M')\n",
    "    monthly_groups = tweets_df.groupby('month')\n",
    "    # Dictionary to hold clusters per month\n",
    "    monthly_clusters = {}\n",
    "    # Number of clusters\n",
    "    n_clusters = 5\n",
    "\n",
    "    for month, group in monthly_groups:\n",
    "        topics = group['topics'].values  # Get topics for the current month  # Get embeddings for the current month\n",
    "        kmeans = KMeans(n_clusters=n_clusters, random_state=0).fit(topics.reshape(-1, 1))\n",
    "        monthly_clusters[month] = kmeans.labels_\n",
    "\n",
    "    # Dictionary to hold clusters per month\n",
    "    monthly_clusters = {}\n",
    "\n",
    "    for month, group in monthly_groups:\n",
    "        topics = group['topics'].values  # Get topics for the current month\n",
    "        kmeans = KMeans(n_clusters=n_clusters, random_state=0).fit(topics)\n",
    "        monthly_clusters[month] = kmeans.labels_\n",
    "\n",
    "    # Function to compare clusters between two months\n",
    "    def compare_clusters(clusters_month1, clusters_month2):\n",
    "        mapping = {}\n",
    "        for cluster in np.unique(clusters_month1):\n",
    "            common_cluster = mode(clusters_month2[clusters_month1 == cluster]).mode[0]\n",
    "            mapping[cluster] = common_cluster\n",
    "        return mapping\n",
    "\n",
    "    # Track changes\n",
    "    cluster_changes = {}\n",
    "    months = sorted(monthly_clusters.keys())\n",
    "\n",
    "    for i in range(len(months) - 1):\n",
    "        month1 = months[i]\n",
    "        month2 = months[i + 1]\n",
    "        mapping = compare_clusters(monthly_clusters[month1], monthly_clusters[month2])\n",
    "        cluster_changes[(month1, month2)] = mapping\n",
    "    # Print the results\n",
    "    for (month1, month2), mapping in cluster_changes.items():\n",
    "        print(f\"Cluster changes from {month1} to {month2}: {mapping}\")"
   ],
   "outputs": [],
   "execution_count": 52
  },
  {
   "cell_type": "code",
   "id": "d62508c9215276fc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T21:25:34.969718Z",
     "start_time": "2024-07-16T21:25:34.934132Z"
    }
   },
   "source": [
    "def clustering_per_topic():\n",
    "    # Convert 'date' to datetime\n",
    "\n",
    "    merged_df = pd.concat(\n",
    "        [replies_df, quotes_df], ignore_index=True\n",
    "    )\n",
    "    merged_df['date'] = pd.to_datetime(merged_df['date'])\n",
    "    #print(merged_df['topics'])\n",
    "\n",
    "    # Extract month and year\n",
    "    merged_df['month_year'] = merged_df['date'].dt.to_period('M')\n",
    "\n",
    "    # Group by month and perform clustering\n",
    "    monthly_clusters = {}\n",
    "    for period, group in merged_df.groupby('month_year'):\n",
    "        # Perform clustering (assuming 'topics' can be vectorized or already is)\n",
    "        vectorized_topics = group['topics'].values.reshape(-1, 1)  # Example vectorization\n",
    "        kmeans = KMeans(n_clusters=2)  # Adjust number of clusters as needed\n",
    "        print(vectorized_topics)\n",
    "        if len(vectorized_topics) > 2:\n",
    "            group['cluster'] = kmeans.fit_predict(vectorized_topics)\n",
    "            monthly_clusters[period] = group\n",
    "\n",
    "    # Create a transition matrix\n",
    "    transition_matrix = {}\n",
    "\n",
    "    # Sort the periods\n",
    "    periods = sorted(monthly_clusters.keys())\n",
    "\n",
    "    print('Number of periods:', len(periods))  # Should be 13 if there are 13 periods\n",
    "\n",
    "    for i in range(len(periods) - 1):\n",
    "        current_period = periods[i]\n",
    "        next_period = periods[i + 1]\n",
    "\n",
    "        current_clusters = monthly_clusters[current_period]\n",
    "        next_clusters = monthly_clusters[next_period]\n",
    "\n",
    "        # Filter users who are present in both periods\n",
    "        common_users = set(current_clusters['user_id']).intersection(set(next_clusters['user_id']))\n",
    "\n",
    "        for user_id in common_users:\n",
    "            current_cluster = current_clusters[current_clusters['user_id'] == user_id]['cluster'].values[0]\n",
    "            next_cluster = next_clusters[next_clusters['user_id'] == user_id]['cluster'].values[0]\n",
    "\n",
    "            if (current_period, current_cluster) not in transition_matrix:\n",
    "                transition_matrix[(current_period, current_cluster)] = {}\n",
    "            if (next_period, next_cluster) not in transition_matrix[(current_period, current_cluster)]:\n",
    "                transition_matrix[(current_period, current_cluster)][(next_period, next_cluster)] = 0\n",
    "\n",
    "            transition_matrix[(current_period, current_cluster)][(next_period, next_cluster)] += 1\n",
    "\n",
    "        # Normalize transitions to proportions\n",
    "        for current_key in transition_matrix:\n",
    "            total_users = sum(transition_matrix[current_key].values())\n",
    "            for next_key in transition_matrix[current_key]:\n",
    "                transition_matrix[current_key][next_key] /= total_users\n",
    "\n",
    "    # Prepare data for Sankey diagram\n",
    "    source = []\n",
    "    target = []\n",
    "    value = []\n",
    "\n",
    "    print('Transition matrix:', transition_matrix)\n",
    "\n",
    "    label_map = {}\n",
    "    label_idx = 0\n",
    "    for period in periods:\n",
    "        for cluster in range(2):  # Assuming there are 2 clusters as specified in KMeans\n",
    "            label_map[(period, cluster)] = label_idx\n",
    "            label_idx += 1\n",
    "\n",
    "    for current_key in transition_matrix:\n",
    "        for next_key in transition_matrix[current_key]:\n",
    "            source.append(label_map[current_key])\n",
    "            target.append(label_map[next_key])\n",
    "            value.append(transition_matrix[current_key][next_key])\n",
    "\n",
    "    # Create labels for the Sankey diagram\n",
    "    labels = [f\"Cluster {cluster}\" for period, cluster in label_map.keys()]\n",
    "\n",
    "    print('Labels:', labels)\n",
    "\n",
    "    # Create the Sankey diagram\n",
    "    fig = go.Figure(data=[go.Sankey(\n",
    "        node=dict(\n",
    "            pad=15,\n",
    "            thickness=20,\n",
    "            line=dict(color=\"black\", width=0.5),\n",
    "            label=labels\n",
    "        ),\n",
    "        link=dict(\n",
    "            source=source,\n",
    "            target=target,\n",
    "            value=value\n",
    "        ))])\n",
    "\n",
    "    fig.update_layout(title_text=\"User Migration Between Clusters\", font_size=10)\n",
    "    fig.show()\n",
    "    fig.write_html(\"User Migration Between Clusters.html\")"
   ],
   "outputs": [],
   "execution_count": 53
  },
  {
   "cell_type": "markdown",
   "id": "52ed49ca84eb98b9",
   "metadata": {},
   "source": [
    "# Visualize tweets documents with Plotly (DataMapPlot)"
   ]
  },
  {
   "cell_type": "code",
   "id": "eadc3d54f790e3c4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T21:25:35.379032Z",
     "start_time": "2024-07-16T21:25:35.367406Z"
    }
   },
   "source": [
    "\n",
    "def visualize_documents_with_dataMapPlot():\n",
    "    topic_model = bertopicModel.fit(tweets_df['translation2'], tweet_embeddings)\n",
    "    fig = topic_model.visualize_document_datamap(tweets_df['translation2'], embeddings=tweet_embeddings)\n",
    "    fig.savefig(\"visualize_tweets_docs_datamapplot.png\", bbox_inches=\"tight\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.16.2\n"
     ]
    }
   ],
   "execution_count": 54
  },
  {
   "cell_type": "markdown",
   "id": "96ed732aecfcaeff",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5172b6e411a5375",
   "metadata": {},
   "source": [
    "# data preparation"
   ]
  },
  {
   "cell_type": "code",
   "id": "d64704dde3f1894d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T21:28:02.325291Z",
     "start_time": "2024-07-16T21:25:35.729797Z"
    }
   },
   "source": [
    "import gzip\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "# Ensure PyArrow is installed\n",
    "try:\n",
    "    import pyarrow\n",
    "\n",
    "    print(pyarrow.__version__)\n",
    "except ImportError:\n",
    "    raise ImportError(\"pyarrow is not installed. Please install it using 'pip install pyarrow'.\")\n",
    "\n",
    "if os.path.exists(\"tweets_df.jsonl.gz\"):\n",
    "    tweets_df = pd.read_json(\"tweets_df.jsonl.gz\", lines=True)\n",
    "    tweet_ids = set(tweets_df[\"id\"])\n",
    "else:\n",
    "    print(f\"The file 'tweets_df.jsonl' does not exist.\")\n",
    "\n",
    "    tweets = []\n",
    "    with gzip.open(\"twitter-politics-tweets.jsonl.gz\", \"rt\") as f:\n",
    "        for line in f:\n",
    "            j = json.loads(line)\n",
    "            if j.get(\"translation2\"):\n",
    "                tweets.append({k: v for k, v in j.items() if k in (\"id\", \"date\", \"translation2\", \"user_id\")})\n",
    "    print(\"tweets count: \", len(tweets))\n",
    "    tweets_df = pd.DataFrame(tweets, dtype=\"string[pyarrow]\")\n",
    "    tweets_df[\"date\"] = pd.to_datetime(tweets_df['date'])\n",
    "    tweet_ids = set(tweets_df[\"id\"])\n",
    "    del tweets\n",
    "\n",
    "if os.path.exists(\"replies_df.jsonl.gz\"):\n",
    "    replies_df = pd.read_json(\"replies_df.jsonl.gz\", lines=True)\n",
    "else:\n",
    "    print(f\"The file 'replies_df.jsonl' does not exist.\")\n",
    "    replies = []\n",
    "    with gzip.open(\"twitter-politics-replies.jsonl.gz\", \"rt\") as f:\n",
    "        for line in f:\n",
    "            j = json.loads(line)\n",
    "            if j.get(\"translation2\") and j[\"in_reply_to_tweet_id\"] and str(j[\"in_reply_to_tweet_id\"]) in tweet_ids:\n",
    "                replies.append(\n",
    "                    {k: v for k, v in j.items() if\n",
    "                     k in (\"id\", \"date\", \"in_reply_to_tweet_id\", \"translation2\", \"user_id\")})\n",
    "    print(\"replies count: \", len(replies))\n",
    "    replies_df = pd.DataFrame(replies).convert_dtypes(dtype_backend='pyarrow')\n",
    "    replies_df[\"date\"] = pd.to_datetime(replies_df['date'])\n",
    "    del replies\n",
    "\n",
    "if os.path.exists(\"quotes_df.jsonl.gz\"):\n",
    "    quotes_df = pd.read_json(\"quotes_df.jsonl.gz\", lines=True)\n",
    "else:\n",
    "    print(f\"The file 'quotes_df.jsonl' does not exist.\")\n",
    "    quotes = []\n",
    "    with gzip.open(\"twitter-politics-quotes.jsonl.gz\", \"rt\") as f:\n",
    "        for line in f:\n",
    "            j = json.loads(line)\n",
    "            if j.get(\"translation2\") and j[\"quoted_tweet_id\"] and str(j[\"quoted_tweet_id\"]) in tweet_ids:\n",
    "                quotes.append(\n",
    "                    {k: v for k, v in j.items() if k in (\"id\", \"date\", \"quoted_tweet_id\", \"translation2\", \"user_id\")})\n",
    "    print(\"quotes count: \", len(quotes))\n",
    "    quotes_df = pd.DataFrame(quotes).convert_dtypes(dtype_backend='pyarrow')\n",
    "    quotes_df[\"date\"] = pd.to_datetime(quotes_df['date'])\n",
    "    del quotes\n",
    "\n",
    "tweets_df = tweets_df[tweets_df['topics'] != -1]\n",
    "replies_df = replies_df[replies_df['topics'] != -1]\n",
    "quotes_df = quotes_df[quotes_df['topics'] != -1]\n",
    "\n",
    "\n",
    "\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.1.0\n"
     ]
    }
   ],
   "execution_count": 55
  },
  {
   "cell_type": "markdown",
   "id": "b4d31a9557ef85ef",
   "metadata": {},
   "source": [
    "# generate embeddings"
   ]
  },
  {
   "cell_type": "code",
   "id": "8b5d6286d3c1ee24",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T21:28:05.101574Z",
     "start_time": "2024-07-16T21:28:02.341569Z"
    }
   },
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "\n",
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")"
   ],
   "outputs": [],
   "execution_count": 56
  },
  {
   "cell_type": "code",
   "id": "e33ecbd40ffc5255",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T21:28:05.133504Z",
     "start_time": "2024-07-16T21:28:05.103959Z"
    }
   },
   "source": [
    "# load tweet embeddings or save a new one if it doesn't exist\n",
    "\n",
    "try:\n",
    "    tweet_embeddings = np.load(\"embeddings-tweets.npy\", mmap_mode=\"r+\")\n",
    "except OSError:\n",
    "    tweet_embeddings = embedding_model.encode(tweets_df[\"translation2\"], show_progress_bar=True)\n",
    "    # save tweet embeddings\n",
    "    with open(\"embeddings-tweets.npy\", \"wb\") as f:\n",
    "        np.save(f, tweet_embeddings, allow_pickle=False)\n"
   ],
   "outputs": [],
   "execution_count": 57
  },
  {
   "cell_type": "code",
   "id": "b3db6d6c25c2e3f7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T21:28:05.140518Z",
     "start_time": "2024-07-16T21:28:05.137439Z"
    }
   },
   "source": [],
   "outputs": [],
   "execution_count": 57
  },
  {
   "cell_type": "code",
   "id": "5da85184406cf2f2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T21:28:05.151993Z",
     "start_time": "2024-07-16T21:28:05.142005Z"
    }
   },
   "source": [
    "# load replies embeddings or save a new one if it doesn't exist\n",
    "\"\"\"\n",
    "try:\n",
    "    reply_embeddings = np.load(\"embeddings-replies.npy\", mmap_mode=\"r+\")\n",
    "except OSError:\n",
    "    reply_embeddings = embedding_model.encode(replies_df[\"translation2\"], show_progress_bar=True)\n",
    "    # save replies embeddings\n",
    "    with open(\"embeddings-replies.npy\", \"wb\") as f:\n",
    "        np.save(f, reply_embeddings, allow_pickle=False)\n",
    "\"\"\""
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntry:\\n    reply_embeddings = np.load(\"embeddings-replies.npy\", mmap_mode=\"r+\")\\nexcept OSError:\\n    reply_embeddings = embedding_model.encode(replies_df[\"translation2\"], show_progress_bar=True)\\n    # save replies embeddings\\n    with open(\"embeddings-replies.npy\", \"wb\") as f:\\n        np.save(f, reply_embeddings, allow_pickle=False)\\n'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 58
  },
  {
   "cell_type": "code",
   "id": "fe4305688b5bd562",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T21:28:05.159872Z",
     "start_time": "2024-07-16T21:28:05.154303Z"
    }
   },
   "source": [
    "# load quotes embeddings or save a new one if it doesn't exist\n",
    "\"\"\"\n",
    "try:\n",
    "    quote_embeddings = np.load(\"embeddings-quotes.npy\", mmap_mode=\"r+\")\n",
    "except OSError:\n",
    "    quote_embeddings = embedding_model.encode(quotes_df[\"translation2\"], show_progress_bar=True)\n",
    "    # save quotes embeddings\n",
    "    with open(\"embeddings-quotes.npy\", \"wb\") as f:\n",
    "        np.save(f, quote_embeddings, allow_pickle=False)\n",
    "\"\"\""
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntry:\\n    quote_embeddings = np.load(\"embeddings-quotes.npy\", mmap_mode=\"r+\")\\nexcept OSError:\\n    quote_embeddings = embedding_model.encode(quotes_df[\"translation2\"], show_progress_bar=True)\\n    # save quotes embeddings\\n    with open(\"embeddings-quotes.npy\", \"wb\") as f:\\n        np.save(f, quote_embeddings, allow_pickle=False)\\n'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 59
  },
  {
   "cell_type": "markdown",
   "id": "39138aafdb83e08b",
   "metadata": {},
   "source": [
    "# Bertopic"
   ]
  },
  {
   "cell_type": "code",
   "id": "890459370d9e8589",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T21:28:06.930092Z",
     "start_time": "2024-07-16T21:28:05.162434Z"
    }
   },
   "source": [
    "\n",
    "\n",
    "from umap import UMAP\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from bertopic import BERTopic\n",
    "from hdbscan import HDBSCAN\n",
    "from nltk.corpus import stopwords\n",
    "from typing import List\n",
    "from bertopic.representation import KeyBERTInspired, MaximalMarginalRelevance\n",
    "\n",
    "\n",
    "def get_stopwords(lang: str = \"english\") -> List[str]:\n",
    "    nltk.download(\"stopwords\")\n",
    "    words = set(stopwords.words(lang))\n",
    "    words |= {\n",
    "        \"unk\",  # unknown token\n",
    "        \"amp\"\n",
    "    }\n",
    "    return list(words)\n",
    "\n",
    "\n",
    "notebook_path = os.path.abspath(\"Notebook.ipynb\")\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "umap_model = UMAP(n_neighbors=3, n_components=3, min_dist=0.05)\n",
    "hdbscan_model = HDBSCAN(min_cluster_size=80, min_samples=40,\n",
    "                        gen_min_span_tree=True,\n",
    "                        prediction_data=True)\n",
    "vectorizer_model = CountVectorizer(ngram_range=(1, 2), stop_words=list(stopwords.words('english')))\n",
    "\n",
    "\n",
    "\n",
    "if os.path.exists('bertopic_model'):\n",
    "    bertopicModel = BERTopic.load('bertopic_model')\n",
    "    print('Load BERTopic')\n",
    "    print(\"finish\")\n",
    "else:\n",
    "    print('Create BERTopic')\n",
    "    embedding_model = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "    umap_model = UMAP(\n",
    "        n_neighbors=15,\n",
    "        n_components=5,\n",
    "        min_dist=0.0,\n",
    "        metric=\"cosine\",\n",
    "        low_memory=True,\n",
    "        random_state=42,\n",
    "    )\n",
    "    hdbscan_model = HDBSCAN(min_cluster_size=500, metric=\"euclidean\",\n",
    "                            prediction_data=False)\n",
    "    vectorizer_model = CountVectorizer(\n",
    "        ngram_range=(1, 3), min_df=10, max_features=10_000,\n",
    "        stop_words=get_stopwords()\n",
    "    )\n",
    "    keybert_model = KeyBERTInspired()\n",
    "    mmr_model = MaximalMarginalRelevance(diversity=0.3)\n",
    "    representation_model = {\n",
    "        \"KeyBERT\": keybert_model,\n",
    "        \"MMR\": mmr_model,\n",
    "    }\n",
    "    bertopicModel = BERTopic(\n",
    "        umap_model=umap_model,\n",
    "        hdbscan_model=hdbscan_model,\n",
    "        embedding_model=embedding_model,\n",
    "        vectorizer_model=vectorizer_model,\n",
    "        representation_model=representation_model,\n",
    "        calculate_probabilities=False,\n",
    "        verbose=True,\n",
    "        top_n_words=10,\n",
    "    )\n",
    "\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/akramchorfi/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/akramchorfi/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create BERTopic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/akramchorfi/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "execution_count": 60
  },
  {
   "cell_type": "code",
   "id": "8781cdc02785c9f2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T21:28:07.147843Z",
     "start_time": "2024-07-16T21:28:06.934070Z"
    }
   },
   "source": [
    "print('size tweets :', len(tweets_df[\"translation2\"].tolist()))\n",
    "#topics_quotes_incoming, _ = bertopicModel.fit_transform(quotes_df[\"translation2\"], quote_embeddings)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size tweets : 101500\n"
     ]
    }
   ],
   "execution_count": 61
  },
  {
   "cell_type": "code",
   "id": "5736607a544907a1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T21:28:07.154454Z",
     "start_time": "2024-07-16T21:28:07.150959Z"
    }
   },
   "source": [],
   "outputs": [],
   "execution_count": 61
  },
  {
   "cell_type": "code",
   "id": "989e7e15f9bcffde",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T21:28:07.172275Z",
     "start_time": "2024-07-16T21:28:07.168102Z"
    }
   },
   "source": [
    "# train on tweet data\n",
    "\n",
    "#topics, probs = bertopicModel.fit_transform(tweets_df[\"translation2\"], tweet_embeddings)"
   ],
   "outputs": [],
   "execution_count": 62
  },
  {
   "cell_type": "code",
   "id": "7b89ca812cf02eed",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T21:28:07.194188Z",
     "start_time": "2024-07-16T21:28:07.173878Z"
    }
   },
   "source": [
    "#topics_replies_incoming, _ = bertopicModel.fit_transform(replies_df[\"translation2\"], reply_embeddings)"
   ],
   "outputs": [],
   "execution_count": 63
  },
  {
   "cell_type": "code",
   "id": "7650b863670fa182",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T21:28:07.203764Z",
     "start_time": "2024-07-16T21:28:07.198113Z"
    }
   },
   "source": [
    "#topics_quotes_incoming, _ = bertopicModel.fit_transform(quotes_df[\"translation2\"], quote_embeddings)"
   ],
   "outputs": [],
   "execution_count": 64
  },
  {
   "cell_type": "markdown",
   "id": "a2e64df3b867db8e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "id": "70a9f78a9a535a3a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T21:28:07.209681Z",
     "start_time": "2024-07-16T21:28:07.205920Z"
    }
   },
   "source": [
    "#tweets_df[\"topics\"] = topics\n",
    "#replies_df[\"topics_incoming\"] = topics_replies_incoming\n",
    "#quotes_df[\"topics_incoming\"] = topics_quotes_incoming"
   ],
   "outputs": [],
   "execution_count": 65
  },
  {
   "cell_type": "code",
   "id": "5b256b5760372a66",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T21:28:07.220485Z",
     "start_time": "2024-07-16T21:28:07.212299Z"
    }
   },
   "source": [
    "# infer replies topics\n",
    "#topics, _ = bertopicModel.transform(replies_df[\"translation2\"], reply_embeddings)"
   ],
   "outputs": [],
   "execution_count": 66
  },
  {
   "cell_type": "code",
   "id": "eabcc5a653bc3df",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T21:28:07.228902Z",
     "start_time": "2024-07-16T21:28:07.223435Z"
    }
   },
   "source": [
    "#replies_df[\"topics\"] = topics"
   ],
   "outputs": [],
   "execution_count": 67
  },
  {
   "cell_type": "code",
   "id": "a73a82b38e452043",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T21:28:07.237320Z",
     "start_time": "2024-07-16T21:28:07.233239Z"
    }
   },
   "source": [
    "# infer quotes topics\n",
    "#topics, _ = bertopicModel.transform(quotes_df[\"translation2\"], quote_embeddings)"
   ],
   "outputs": [],
   "execution_count": 68
  },
  {
   "cell_type": "code",
   "id": "52b164da6019029f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T21:28:07.246101Z",
     "start_time": "2024-07-16T21:28:07.240592Z"
    }
   },
   "source": [
    "#quotes_df[\"topics\"] = topics"
   ],
   "outputs": [],
   "execution_count": 69
  },
  {
   "cell_type": "code",
   "id": "92c012dd6fb4aa81",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T21:28:07.254758Z",
     "start_time": "2024-07-16T21:28:07.249710Z"
    }
   },
   "source": [
    "# infer tweet topics\n",
    "#topics, _ = bertopicModel.transform(tweets_df[\"translation2\"], tweet_embeddings)\n",
    "#tweets_df[\"topics_incoming\"] = topics"
   ],
   "outputs": [],
   "execution_count": 70
  },
  {
   "cell_type": "markdown",
   "id": "a5d778a709a4e2ad",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "id": "7a0d4b4b7964e83d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T21:28:07.262409Z",
     "start_time": "2024-07-16T21:28:07.257435Z"
    }
   },
   "source": [
    "# save model and dataframes\n",
    "#bertopicModel.save(\"bertopic_model\")\n",
    "#tweets_df.to_json(\"tweets_df.jsonl.gz\", orient=\"records\", lines=True)\n",
    "#replies_df.to_json(\"replies_df.jsonl.gz\", orient=\"records\", lines=True)\n",
    "#quotes_df.to_json(\"quotes_df.jsonl.gz\", orient=\"records\", lines=True)"
   ],
   "outputs": [],
   "execution_count": 71
  },
  {
   "cell_type": "markdown",
   "id": "349033438509fd76",
   "metadata": {},
   "source": [
    "# split dataframes by time"
   ]
  },
  {
   "cell_type": "code",
   "id": "bb4465515e19d8d4",
   "metadata": {},
   "source": [
    "from bertopic import BERTopic\n",
    "\n",
    "G = nx.DiGraph()\n",
    "colors = [\"red\", \"green\", \"blue\", \"yellow\", \"orange\", \"purple\", \"pink\", \"black\", \"white\", \"brown\", \"gray\"]\n",
    "nt = Network(height='800px', width='100%')\n",
    "nt1 = Network(height='800px', width='100%')\n",
    "nt2 = Network(height='800px', width='100%')\n",
    "nt3 = Network(height='800px', width='100%')\n",
    "# Segment data into time intervals (e.g., monthly)\n",
    "time_intervals_tweets = pd.date_range(start=tweets_df['date'].min(), end=tweets_df['date'].max(), freq='M')\n",
    "time_intervals_replies = pd.date_range(start=replies_df['date'].min(), end=replies_df['date'].max(), freq='M')\n",
    "time_intervals_quotes = pd.date_range(start=quotes_df['date'].min(), end=quotes_df['date'].max(), freq='M')\n",
    "df_overtime = pd.DataFrame(columns=['date', 'result'])\n",
    "#calculate_conditional_probability_for_topics_transitions_between_users()\n",
    "print(set(replies_df['topics'].tolist()))\n",
    "#apply_granger_causality_between_topics_from_tweets()\n",
    "#build_network_analysis()\n",
    "#visualize_quotes_counter_over_time()\n",
    "#test_clustering()\n",
    "#find_relations_politicians()o\n",
    "#track_topic_clusters()\n",
    "#clustering_per_topic()\n",
    "find_relation_between_topics_based_on_user_communities()\n",
    "#visualize_documents_with_dataMapPlot()\n",
    "#apply_granger_causality_between_topics_from_tweets()\n",
    "#apply_granger_causality_between_topics_from_tweets_and_replies()\n",
    "#visualize_outgoing_relation_using_plotly()\n",
    "#visualize_outgoing_relation_using_plotly()\n",
    "#visualize_outgoing_relation_using_plotly()\n",
    "\n",
    "\"\"\"\n",
    "df_overtime_incoming = pd.DataFrame(columns=['date', 'result'])\n",
    "for time_interval in time_intervals_tweets:\n",
    "    # Filter data for the current time interval\n",
    "    data_interval = tweets_df[\n",
    "        (tweets_df['date'] >= time_interval) & (tweets_df['date'] < time_interval + pd.DateOffset(months=1))]\n",
    "    result_outgoing, result_incoming = find_topics_by_tweets(data_interval)\n",
    "    df_overtime.loc[len(df_overtime)] = {'date': time_interval, 'result': result_outgoing.to_dict('records')}\n",
    "    df_overtime['date'] = df_overtime['date'].astype(str)\n",
    "    df_overtime_incoming.loc[len(df_overtime_incoming)] = {'date': time_interval,\n",
    "                                                           'result': result_incoming.to_dict('records')}\n",
    "    df_overtime_incoming['date'] = df_overtime_incoming['date'].astype(str)\n",
    "    with open(os.path.join(os.path.dirname(notebook_path), 'result_relations.json'), 'w') as json_file:\n",
    "        json.dump(df_overtime.to_dict('records'), json_file, default=str)\n",
    "print(df_overtime.to_dict('records'))\n",
    "\"\"\"\n",
    "#visualize_evolution(df_overtime)\n",
    "#visualize_evolution_main_topic_overtime(df_overtime)"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
