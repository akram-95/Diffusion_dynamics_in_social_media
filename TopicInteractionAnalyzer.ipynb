{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "import json\n",
    "import textwrap\n",
    "from itertools import combinations\n",
    "from IPython.display import display\n",
    "\n",
    "import networkx as nx\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from bertopic import BERTopic\n",
    "from gensim import corpora, models\n",
    "from matplotlib import ticker\n",
    "from matplotlib.ticker import MultipleLocator, ScalarFormatter\n",
    "from pandas import Timestamp\n",
    "\n",
    "from pyvis.network import Network\n",
    "from scipy.stats import pearsonr\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from typing import List\n",
    "from umap import UMAP\n",
    "from hdbscan import HDBSCAN\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.corpus import stopwords\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import seaborn as sns\n",
    "from api.utils import parse_json, identify_similarity_sentence_topics, get_result, calculate_cosine_sim, equal, \\\n",
    "    choose_colors\n",
    "from datetime import datetime, timedelta\n",
    "import os\n"
   ],
   "id": "404e7b86ae211b8c"
  },
  {
   "cell_type": "code",
   "id": "6c6b13dee5d1f410",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-29T23:13:14.812818Z",
     "start_time": "2024-05-29T23:13:14.799874Z"
    }
   },
   "source": [
    "\n",
    "def normalize_topics_relation_weights(df: pd.DataFrame):\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        if len(row['replies_quotes_topics']) > 0:\n",
    "            max_num = max([i['count'] for i in row['replies_quotes_topics']])\n",
    "            sum_num = sum([i['count'] for i in row['replies_quotes_topics']])\n",
    "            min_num = min([i['count'] for i in row['replies_quotes_topics']])\n",
    "            if len(row['replies_quotes_topics']) == 1:\n",
    "                row['replies_quotes_topics'][0]['count'] = 1\n",
    "            else:\n",
    "                for index_count, count in enumerate(row['replies_quotes_topics']):\n",
    "                    normalized_number = (count['count'] / sum_num)\n",
    "                    row['replies_quotes_topics'][index_count]['count'] = normalized_number\n",
    "                    \"\"\"\n",
    "                    if min_num != max_num:\n",
    "                        # normalized_number = (count['count'] - min_num) / (max_num - min_num)\n",
    "                        normalized_number = (count['count'] / sum_num)\n",
    "                        row['replies_quotes_topics'][index_count]['count'] = normalized_number\n",
    "                    else:\n",
    "                        row['replies_quotes_topics'][index_count]['count'] = 0\n",
    "                    \"\"\"\n",
    "        df.iloc[index] = row\n"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "38bc622717d416f5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-29T23:13:14.827735Z",
     "start_time": "2024-05-29T23:13:14.816249Z"
    }
   },
   "source": [
    "\n",
    "def find_topics_by_tweets(inputDataFrame: pd.DataFrame):\n",
    "    df = pd.DataFrame(columns=['topic', 'replies_quotes_topics'])\n",
    "    topics, probs = bertopicModel.fit_transform(inputDataFrame['translation2'])\n",
    "\n",
    "    for index, topic_id in enumerate(topics):\n",
    "\n",
    "        topic_info = bertopicModel.get_topic_info(topic_id)\n",
    "        replies = [sub['translation2'] for sub in inputDataFrame.iloc[index]['replies']]\n",
    "        quotes = [sub['translation2'] for sub in inputDataFrame.iloc[index]['quotes']]\n",
    "        replies_quotes = replies + quotes\n",
    "\n",
    "        if len(replies_quotes) > 10:\n",
    "            replies_quotes_topics = []\n",
    "            try:\n",
    "                replies_quotes_topics = bertopicModel.transform(replies_quotes)[0]\n",
    "                print(\"Main Topic Id: \", topic_id)\n",
    "                print(\"Derived Topics Id: \", replies_quotes_topics)\n",
    "            except Exception as e:\n",
    "                print(f\"An error occurred during topic modeling: {e}\")\n",
    "            replies_quotes_topics_dict = []\n",
    "            for replies_quotes_topic in set(replies_quotes_topics):\n",
    "                count = replies_quotes_topics.count(replies_quotes_topic)\n",
    "                replies_quotes_topics_dict.append({'count': count,\n",
    "                                                   'replies_quotes_topic': replies_quotes_topic})\n",
    "            df.loc[len(df)] = {'topic': topic_id, 'replies_quotes_topics': replies_quotes_topics_dict}\n",
    "    normalize_topics_relation_weights(df)\n",
    "    return df\n",
    "\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "9d478f3fefa8a1f3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-29T23:13:14.855426Z",
     "start_time": "2024-05-29T23:13:14.834892Z"
    }
   },
   "source": [
    "\n",
    "def visualize_evolution(edge_weights_over_time: pd.DataFrame):\n",
    "    # Plotting\n",
    "\n",
    "    print(\"topics connection with topics from replies/ quotes after normalization\")\n",
    "    display(edge_weights_over_time.to_dict('records'))\n",
    "\n",
    "    for index, row in edge_weights_over_time.iterrows():\n",
    "        scatter_plots = []\n",
    "        dates = []\n",
    "        main_topics_id = []\n",
    "        weights = []\n",
    "        derived_topics_id = []\n",
    "        time_periods = row['date']\n",
    "        dates.append(time_periods)\n",
    "        result = row['result']\n",
    "        seen = set()\n",
    "        plt.figure(num=str(time_periods), figsize=(10, 6))\n",
    "        for j, item in enumerate(result):\n",
    "            main_topic_id = item['topic']\n",
    "            derived_topics = item['replies_quotes_topics']\n",
    "            for k, derived_topic in enumerate(derived_topics):\n",
    "                \"\"\"and (main_topic_id, derived_topic['replies_quotes_topic'], time_periods) not in seen\"\"\"\n",
    "                if main_topic_id != derived_topic['replies_quotes_topic'] and (\n",
    "                        main_topic_id, derived_topic['replies_quotes_topic']) not in seen:\n",
    "                    \"\"\"plt.plot(main_topic_id, derived_topic['replies_quotes_topic'],\n",
    "                              label=f\"Main topic_id: {main_topic_id} to Derived topic_id: \"\n",
    "                                    f\"{derived_topic['replies_quotes_topic']} with weight: {derived_topic['count']}\",\n",
    "                              marker='o')\n",
    "                     \"\"\"\n",
    "                    scatter = plt.scatter(main_topic_id, derived_topic['replies_quotes_topic'],\n",
    "                                          s=derived_topic['count'] * 100, c='b',\n",
    "                                          alpha=0.5)\n",
    "                    main_topics_id.append(main_topic_id)\n",
    "                    seen.add((\n",
    "                        main_topic_id, derived_topic['replies_quotes_topic']))\n",
    "                    derived_topics_id.append(derived_topic['replies_quotes_topic'])\n",
    "                    scatter_plots.append(scatter)\n",
    "                    weights.append(derived_topic['count'])\n",
    "                    \"\"\"\"\n",
    "                    plt.annotate(\n",
    "                        f\"main topic id: {main_topic_id} to derived topic id: {derived_topic['replies_quotes_topic']}\",\n",
    "                        (time_periods, derived_topic['count']),\n",
    "                        textcoords=\"offset points\",\n",
    "                        xytext=(0, 10),\n",
    "                        ha='center')\"\"\"\n",
    "                    plt.text(main_topic_id, derived_topic['replies_quotes_topic'],\n",
    "                             f'Weight: {round(derived_topic[\"count\"], 1)}', fontsize=8, ha='center',\n",
    "                             va='bottom')\n",
    "\n",
    "        plt.gca().xaxis.set_major_locator(MultipleLocator())\n",
    "        plt.gca().yaxis.set_major_locator(MultipleLocator())\n",
    "\n",
    "        plt.title(f\"Relation between Main topics and derived topics from Replies/Quotes for date {time_periods} \")\n",
    "        plt.xlabel(\"Main Topic\")\n",
    "        plt.ylabel(\"Derived Topic from from Replies/Quotes\")\n",
    "        # plt.yticks(list(set(weights)))\n",
    "\n",
    "        # Show plot\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Display the resulting DataFrame"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "14bbe95956b955ce",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-29T23:17:29.724954Z",
     "start_time": "2024-05-29T23:13:14.859052Z"
    }
   },
   "source": [
    "\n",
    "\n",
    "notebook_path = os.path.abspath(\"Notebook.ipynb\")\n",
    "\n",
    "nltk.download('punkt')\n",
    "G = nx.DiGraph()\n",
    "colors = [\"red\", \"green\", \"blue\", \"yellow\", \"orange\", \"purple\", \"pink\", \"black\", \"white\", \"brown\", \"gray\"]\n",
    "nt = Network(height=\"750px\", width=\"100%\", bgcolor=\"#222222\", font_color=\"white\", filter_menu=True)\n",
    "nltk.download('stopwords')\n",
    "\n",
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "umap_model = UMAP(n_neighbors=3, n_components=3, min_dist=0.05)\n",
    "hdbscan_model = HDBSCAN(min_cluster_size=80, min_samples=40,\n",
    "                        gen_min_span_tree=True,\n",
    "                        prediction_data=True)\n",
    "vectorizer_model = CountVectorizer(ngram_range=(1, 2), stop_words=list(stopwords.words('english')))\n",
    "\n",
    "bertopicModel = BERTopic(\n",
    "    umap_model=umap_model,\n",
    "    hdbscan_model=hdbscan_model,\n",
    "    embedding_model=embedding_model,\n",
    "    vectorizer_model=vectorizer_model,\n",
    "    top_n_words=10,\n",
    "    language='english',\n",
    "    calculate_probabilities=True,\n",
    "    verbose=True,\n",
    ")\n",
    "stopwords1 = list(stopwords.words('english')) + ['http', 'https', 'amp', 'com']\n",
    "\n",
    "# we add this to remove stopwords that can pollute topcs\n",
    "vectorizer_model = CountVectorizer(ngram_range=(1, 2), stop_words=stopwords1)\n",
    "print(\"Data Parsing\")\n",
    "tweets_parsed = parse_json(os.path.join(os.path.dirname(notebook_path), 'twitter-politics-tweets.jsonl'))[0:10000]\n",
    "\n",
    "replies_parsed = parse_json(os.path.join(os.path.dirname(notebook_path), 'twitter-politics-tweets_reply.jsonl'))\n",
    "\n",
    "quotes_parsed = parse_json(os.path.join(os.path.dirname(notebook_path), 'twitter-politics-tweets_quote.jsonl'))\n",
    "\n",
    "print(\"Data Parsed\")\n",
    "# Convert lists of dictionaries into DataFrames\n",
    "tweets_df = pd.DataFrame(tweets_parsed)\n",
    "tweets_df['id'] = tweets_df['id'].astype('object')\n",
    "replies_df = pd.DataFrame(replies_parsed)\n",
    "prefix = 'reply_'\n",
    "\"\"\"\n",
    "replies_df = replies_df.rename(\n",
    "    columns={old_key: prefix + old_key if old_key != 'in_reply_to_tweet_id' else old_key for old_key in\n",
    "             replies_df.columns})\n",
    "\"\"\"\n",
    "replies_df['id'] = replies_df['id'].astype('object')\n",
    "replies_df['in_reply_to_tweet_id'] = replies_df['in_reply_to_tweet_id'].astype('object')\n",
    "quotes_df = pd.DataFrame(quotes_parsed)\n",
    "prefix = 'quote_'\n",
    "\"\"\"\n",
    "quotes_df = quotes_df.rename(\n",
    "    columns={old_key: prefix + old_key if old_key != 'quoted_tweet_id' else old_key for old_key in\n",
    "             quotes_df.columns})\n",
    "\"\"\"\n",
    "quotes_df['id'] = quotes_df['id'].astype('object')\n",
    "quotes_df['quoted_tweet_id'] = quotes_df['quoted_tweet_id'].astype('object')\n",
    "\n",
    "# Group replies by tweet ID and aggregate into a list of dictionaries\n",
    "grouped_replies = replies_df.groupby('in_reply_to_tweet_id').apply(lambda x: x.to_dict(orient='records')).reset_index()\n",
    "\n",
    "# Rename the resulting column\n",
    "grouped_replies = grouped_replies.rename(columns={0: 'replies'})\n",
    "\n",
    "# Group replies by tweet ID and aggregate into a list of dictionaries\n",
    "grouped_quotes = quotes_df.groupby('quoted_tweet_id').apply(lambda x: x.to_dict(orient='records')).reset_index()\n",
    "\n",
    "# Rename the resulting column\n",
    "grouped_quotes = grouped_quotes.rename(columns={0: 'quotes'})\n",
    "\n",
    "# Merge replies and quotes with tweets based on tweet ID\n",
    "merged_df = pd.merge(tweets_df, grouped_replies, left_on='id', right_on='in_reply_to_tweet_id', how='left')\n",
    "\n",
    "result_df = pd.merge(merged_df, grouped_quotes, left_on='id', right_on='quoted_tweet_id',\n",
    "                     how='left')\n",
    "\n",
    "\n",
    "# Define a function to replace NaN values with an empty list\n",
    "def fillna_with_empty_list(value):\n",
    "    if isinstance(value, list):\n",
    "        return value\n",
    "    elif pd.isna(value):\n",
    "        return []\n",
    "    else:\n",
    "        return [value]\n",
    "\n",
    "\n",
    "result_df['quotes'] = result_df['quotes'].apply(fillna_with_empty_list)\n",
    "result_df['replies'] = result_df['replies'].apply(fillna_with_empty_list)\n",
    "\n",
    "print(\"input data frame :\")\n",
    "display(result_df)\n",
    "result_df['date'] = pd.to_datetime(result_df['date'])\n",
    "# Segment data into time intervals (e.g., monthly)\n",
    "time_intervals = pd.date_range(start=result_df['date'].min(), end=result_df['date'].max(), freq='M')\n",
    "df_overtime = pd.DataFrame(columns=['date', 'result'])\n",
    "for time_interval in time_intervals:\n",
    "    # Filter data for the current time interval\n",
    "    data_interval = result_df[\n",
    "        (result_df['date'] >= time_interval) & (result_df['date'] < time_interval + pd.DateOffset(months=1))]\n",
    "    result = find_topics_by_tweets(data_interval)\n",
    "    df_overtime.loc[len(df_overtime)] = {'date': time_interval, 'result': result.to_dict('records')}\n",
    "    df_overtime['date'] = df_overtime['date'].astype(str)\n",
    "    with open(os.path.join(os.path.dirname(notebook_path), 'result_relations.json'), 'w') as json_file:\n",
    "        json.dump(df_overtime.to_dict('records'), json_file, default=str)\n",
    "visualize_evolution(df_overtime)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/akramchorfi/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/akramchorfi/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Parsing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "30aefce9-9d52-4a1c-8398-c0c95d23069e",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
